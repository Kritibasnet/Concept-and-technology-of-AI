{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# To do Task 1\n"
      ],
      "metadata": {
        "id": "j-s1_jHXyR0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the dataset (replace 'your_dataset.csv' with your actual dataset path)\n",
        "df = pd.read_csv('/content/drive/MyDrive/Concepts and technology  of AI/ student.csv')\n",
        "\n",
        "# Observe the first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_4_RrDHzP46",
        "outputId": "f77afca4-3b08-44e1-ee2e-c44d82ca1ebe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print top 5 rows\n",
        "print(\"Top 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print bottom 5 rows\n",
        "print(\"Bottom 5 rows:\")\n",
        "print(df.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruzCfTv5zdEQ",
        "outputId": "81e82f7a-09bd-48a3-ff60-d7627085dd18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "Bottom 5 rows:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get concise information about the dataset\n",
        "print(\"Dataset Information:\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8YFEAAHzhVZ",
        "outputId": "40a35a66-6578-4382-bde5-b0bb0e156a71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get descriptive statistics for the numerical columns\n",
        "print(\"Descriptive Statistics:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haaMwusyzmzE",
        "outputId": "c03c5b1a-4cad-4cb6-8af0-4117401f64ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into Features (X) and Label (Y)\n",
        "X = df.drop(columns=['Math'])  # Features: Reading and Writing\n",
        "Y = df['Math']  # Label: Math\n",
        "\n",
        "# Print the shapes of X and Y to confirm\n",
        "print(\"Shape of X (Features):\", X.shape)\n",
        "print(\"Shape of Y (Label):\", Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Gv7t55ztkf",
        "outputId": "9be26784-b2d8-44c8-d4ba-24394ea7e0a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (Features): (1000, 2)\n",
            "Shape of Y (Label): (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To do Task 2"
      ],
      "metadata": {
        "id": "pA-oRodZz1r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Number of features (d) - based on your dataset (Reading and Writing are features)\n",
        "d = X.shape[1]  # In this case, X has 2 features (Reading and Writing)\n",
        "\n",
        "# Initialize W with random values (just as an example)\n",
        "W = np.random.rand(d, 1)  # W is a d x 1 vector\n",
        "print(\"Weight Vector W:\")\n",
        "print(W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5RW7SxU0BM9",
        "outputId": "a8ec8953-dc41-47eb-b4eb-2c2d52e186e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Vector W:\n",
            "[[0.63386348]\n",
            " [0.74721611]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature matrix X: 2 features (Reading and Writing) and 5 samples\n",
        "X = df[['Reading', 'Writing']].T  # Transpose to get the shape (d x n)\n",
        "X = X.values  # Convert to numpy array\n",
        "print(\"Feature Matrix X:\")\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLfiSfCU0F92",
        "outputId": "1a15952e-d0fd-46d1-8418-ec73e30c5e4d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Matrix X:\n",
            "[[68 81 80 ... 87 82 66]\n",
            " [63 72 78 ... 94 78 72]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output vector Y: target values for Math\n",
        "Y = df['Math'].values.reshape(-1, 1)  # Reshape to make it a column vector (n x 1)\n",
        "print(\"Output Vector Y:\")\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpOb5G670KLs",
        "outputId": "88cd299a-8cc3-4e64-850d-c952012ff15a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Vector Y:\n",
            "[[ 48]\n",
            " [ 62]\n",
            " [ 79]\n",
            " [ 76]\n",
            " [ 59]\n",
            " [ 69]\n",
            " [ 70]\n",
            " [ 46]\n",
            " [ 61]\n",
            " [ 86]\n",
            " [ 62]\n",
            " [ 72]\n",
            " [ 56]\n",
            " [ 81]\n",
            " [ 61]\n",
            " [ 49]\n",
            " [ 60]\n",
            " [ 45]\n",
            " [ 71]\n",
            " [ 75]\n",
            " [ 66]\n",
            " [ 57]\n",
            " [ 67]\n",
            " [ 63]\n",
            " [ 68]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 56]\n",
            " [ 82]\n",
            " [ 64]\n",
            " [ 71]\n",
            " [ 69]\n",
            " [ 64]\n",
            " [ 76]\n",
            " [ 61]\n",
            " [ 47]\n",
            " [ 50]\n",
            " [ 75]\n",
            " [ 69]\n",
            " [ 42]\n",
            " [ 73]\n",
            " [ 78]\n",
            " [ 65]\n",
            " [ 51]\n",
            " [ 61]\n",
            " [ 69]\n",
            " [ 51]\n",
            " [ 64]\n",
            " [ 90]\n",
            " [ 58]\n",
            " [100]\n",
            " [ 73]\n",
            " [ 62]\n",
            " [ 45]\n",
            " [ 47]\n",
            " [ 53]\n",
            " [ 62]\n",
            " [ 49]\n",
            " [ 77]\n",
            " [ 70]\n",
            " [ 60]\n",
            " [ 82]\n",
            " [100]\n",
            " [ 72]\n",
            " [ 62]\n",
            " [ 87]\n",
            " [ 56]\n",
            " [ 96]\n",
            " [ 68]\n",
            " [ 66]\n",
            " [ 68]\n",
            " [ 59]\n",
            " [ 52]\n",
            " [ 60]\n",
            " [ 60]\n",
            " [ 46]\n",
            " [ 62]\n",
            " [ 30]\n",
            " [ 83]\n",
            " [ 69]\n",
            " [ 82]\n",
            " [ 64]\n",
            " [ 57]\n",
            " [ 71]\n",
            " [ 37]\n",
            " [ 69]\n",
            " [ 72]\n",
            " [ 73]\n",
            " [ 70]\n",
            " [ 75]\n",
            " [ 54]\n",
            " [ 71]\n",
            " [ 60]\n",
            " [ 81]\n",
            " [ 58]\n",
            " [ 54]\n",
            " [ 49]\n",
            " [ 64]\n",
            " [ 84]\n",
            " [ 84]\n",
            " [ 33]\n",
            " [ 51]\n",
            " [ 68]\n",
            " [ 32]\n",
            " [ 43]\n",
            " [ 73]\n",
            " [ 82]\n",
            " [ 47]\n",
            " [ 74]\n",
            " [ 36]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 92]\n",
            " [ 75]\n",
            " [ 43]\n",
            " [ 68]\n",
            " [ 49]\n",
            " [ 69]\n",
            " [ 91]\n",
            " [ 83]\n",
            " [ 65]\n",
            " [ 79]\n",
            " [ 74]\n",
            " [ 81]\n",
            " [ 41]\n",
            " [ 48]\n",
            " [ 31]\n",
            " [ 53]\n",
            " [ 87]\n",
            " [ 94]\n",
            " [ 77]\n",
            " [ 57]\n",
            " [ 64]\n",
            " [ 55]\n",
            " [ 61]\n",
            " [ 78]\n",
            " [ 94]\n",
            " [ 88]\n",
            " [ 77]\n",
            " [ 90]\n",
            " [ 55]\n",
            " [ 83]\n",
            " [ 68]\n",
            " [ 66]\n",
            " [ 56]\n",
            " [ 65]\n",
            " [ 84]\n",
            " [ 90]\n",
            " [ 80]\n",
            " [ 47]\n",
            " [ 67]\n",
            " [ 79]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 62]\n",
            " [ 44]\n",
            " [ 74]\n",
            " [ 67]\n",
            " [ 46]\n",
            " [ 71]\n",
            " [ 73]\n",
            " [ 49]\n",
            " [ 42]\n",
            " [ 55]\n",
            " [ 73]\n",
            " [ 75]\n",
            " [ 96]\n",
            " [ 66]\n",
            " [ 56]\n",
            " [100]\n",
            " [ 53]\n",
            " [ 68]\n",
            " [ 72]\n",
            " [ 55]\n",
            " [ 59]\n",
            " [ 76]\n",
            " [ 84]\n",
            " [ 68]\n",
            " [ 61]\n",
            " [ 53]\n",
            " [ 73]\n",
            " [ 61]\n",
            " [ 73]\n",
            " [ 63]\n",
            " [ 51]\n",
            " [ 70]\n",
            " [ 69]\n",
            " [ 61]\n",
            " [ 88]\n",
            " [ 90]\n",
            " [ 60]\n",
            " [100]\n",
            " [ 68]\n",
            " [ 85]\n",
            " [ 41]\n",
            " [ 70]\n",
            " [ 91]\n",
            " [ 68]\n",
            " [ 63]\n",
            " [ 81]\n",
            " [ 89]\n",
            " [ 66]\n",
            " [ 59]\n",
            " [ 80]\n",
            " [ 87]\n",
            " [ 30]\n",
            " [ 63]\n",
            " [ 43]\n",
            " [ 73]\n",
            " [ 49]\n",
            " [ 65]\n",
            " [ 72]\n",
            " [ 83]\n",
            " [ 46]\n",
            " [ 65]\n",
            " [100]\n",
            " [ 40]\n",
            " [ 73]\n",
            " [ 58]\n",
            " [ 87]\n",
            " [ 61]\n",
            " [ 71]\n",
            " [ 72]\n",
            " [ 81]\n",
            " [ 67]\n",
            " [ 69]\n",
            " [ 53]\n",
            " [ 70]\n",
            " [ 56]\n",
            " [ 84]\n",
            " [ 44]\n",
            " [ 63]\n",
            " [ 82]\n",
            " [ 61]\n",
            " [ 68]\n",
            " [ 75]\n",
            " [ 78]\n",
            " [ 32]\n",
            " [ 62]\n",
            " [ 84]\n",
            " [ 65]\n",
            " [ 41]\n",
            " [ 57]\n",
            " [ 54]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 53]\n",
            " [ 30]\n",
            " [ 63]\n",
            " [ 36]\n",
            " [ 85]\n",
            " [ 66]\n",
            " [ 45]\n",
            " [ 69]\n",
            " [ 71]\n",
            " [ 70]\n",
            " [ 31]\n",
            " [ 61]\n",
            " [ 73]\n",
            " [ 66]\n",
            " [ 63]\n",
            " [ 64]\n",
            " [ 63]\n",
            " [ 19]\n",
            " [ 64]\n",
            " [ 96]\n",
            " [ 59]\n",
            " [ 59]\n",
            " [ 75]\n",
            " [ 51]\n",
            " [100]\n",
            " [ 45]\n",
            " [ 67]\n",
            " [ 34]\n",
            " [ 82]\n",
            " [ 62]\n",
            " [ 59]\n",
            " [ 84]\n",
            " [ 80]\n",
            " [ 65]\n",
            " [ 71]\n",
            " [ 35]\n",
            " [ 46]\n",
            " [ 72]\n",
            " [ 70]\n",
            " [ 80]\n",
            " [ 72]\n",
            " [ 61]\n",
            " [ 39]\n",
            " [ 53]\n",
            " [ 80]\n",
            " [ 73]\n",
            " [ 68]\n",
            " [ 73]\n",
            " [ 57]\n",
            " [ 56]\n",
            " [ 46]\n",
            " [ 48]\n",
            " [ 78]\n",
            " [ 62]\n",
            " [ 89]\n",
            " [ 89]\n",
            " [ 65]\n",
            " [ 49]\n",
            " [ 65]\n",
            " [ 71]\n",
            " [ 70]\n",
            " [ 48]\n",
            " [ 75]\n",
            " [ 83]\n",
            " [ 51]\n",
            " [ 77]\n",
            " [ 51]\n",
            " [ 79]\n",
            " [ 73]\n",
            " [ 72]\n",
            " [ 68]\n",
            " [ 74]\n",
            " [ 58]\n",
            " [ 70]\n",
            " [ 52]\n",
            " [ 70]\n",
            " [ 71]\n",
            " [ 79]\n",
            " [ 71]\n",
            " [ 73]\n",
            " [ 89]\n",
            " [ 54]\n",
            " [ 90]\n",
            " [ 88]\n",
            " [ 72]\n",
            " [ 69]\n",
            " [ 61]\n",
            " [ 46]\n",
            " [ 98]\n",
            " [ 47]\n",
            " [ 71]\n",
            " [ 70]\n",
            " [ 83]\n",
            " [ 51]\n",
            " [ 85]\n",
            " [ 56]\n",
            " [ 98]\n",
            " [ 53]\n",
            " [ 47]\n",
            " [ 86]\n",
            " [ 93]\n",
            " [ 62]\n",
            " [ 66]\n",
            " [ 79]\n",
            " [ 56]\n",
            " [ 41]\n",
            " [ 79]\n",
            " [ 70]\n",
            " [ 89]\n",
            " [ 33]\n",
            " [ 78]\n",
            " [ 77]\n",
            " [ 58]\n",
            " [ 66]\n",
            " [ 72]\n",
            " [ 94]\n",
            " [ 58]\n",
            " [ 55]\n",
            " [ 65]\n",
            " [ 72]\n",
            " [ 46]\n",
            " [ 76]\n",
            " [ 67]\n",
            " [ 71]\n",
            " [ 88]\n",
            " [ 72]\n",
            " [ 66]\n",
            " [ 79]\n",
            " [ 73]\n",
            " [ 49]\n",
            " [ 58]\n",
            " [ 72]\n",
            " [ 78]\n",
            " [ 95]\n",
            " [ 87]\n",
            " [ 95]\n",
            " [ 64]\n",
            " [ 71]\n",
            " [ 80]\n",
            " [ 72]\n",
            " [ 64]\n",
            " [ 46]\n",
            " [ 63]\n",
            " [ 83]\n",
            " [ 61]\n",
            " [ 60]\n",
            " [ 47]\n",
            " [ 82]\n",
            " [ 59]\n",
            " [ 49]\n",
            " [ 80]\n",
            " [ 61]\n",
            " [ 72]\n",
            " [ 64]\n",
            " [ 92]\n",
            " [ 59]\n",
            " [ 69]\n",
            " [ 54]\n",
            " [ 74]\n",
            " [ 78]\n",
            " [ 69]\n",
            " [ 57]\n",
            " [ 55]\n",
            " [ 63]\n",
            " [ 89]\n",
            " [100]\n",
            " [ 60]\n",
            " [ 76]\n",
            " [ 73]\n",
            " [ 64]\n",
            " [ 40]\n",
            " [ 79]\n",
            " [ 90]\n",
            " [ 71]\n",
            " [ 47]\n",
            " [ 48]\n",
            " [ 78]\n",
            " [ 55]\n",
            " [ 76]\n",
            " [ 91]\n",
            " [ 39]\n",
            " [ 35]\n",
            " [ 60]\n",
            " [ 89]\n",
            " [ 82]\n",
            " [ 44]\n",
            " [ 89]\n",
            " [ 87]\n",
            " [ 45]\n",
            " [ 66]\n",
            " [ 81]\n",
            " [ 79]\n",
            " [ 71]\n",
            " [ 43]\n",
            " [ 79]\n",
            " [ 81]\n",
            " [ 64]\n",
            " [ 91]\n",
            " [ 62]\n",
            " [ 78]\n",
            " [ 90]\n",
            " [ 46]\n",
            " [ 81]\n",
            " [ 65]\n",
            " [ 58]\n",
            " [ 73]\n",
            " [ 73]\n",
            " [ 58]\n",
            " [ 46]\n",
            " [ 77]\n",
            " [ 42]\n",
            " [ 82]\n",
            " [ 64]\n",
            " [ 87]\n",
            " [ 60]\n",
            " [ 33]\n",
            " [ 60]\n",
            " [ 56]\n",
            " [ 73]\n",
            " [ 64]\n",
            " [ 64]\n",
            " [ 81]\n",
            " [ 60]\n",
            " [ 79]\n",
            " [ 76]\n",
            " [ 83]\n",
            " [ 65]\n",
            " [ 76]\n",
            " [ 78]\n",
            " [ 71]\n",
            " [ 73]\n",
            " [ 67]\n",
            " [ 64]\n",
            " [ 48]\n",
            " [ 82]\n",
            " [ 58]\n",
            " [ 74]\n",
            " [ 71]\n",
            " [ 60]\n",
            " [ 81]\n",
            " [ 47]\n",
            " [ 76]\n",
            " [ 55]\n",
            " [ 80]\n",
            " [ 79]\n",
            " [ 59]\n",
            " [ 62]\n",
            " [ 86]\n",
            " [ 89]\n",
            " [ 53]\n",
            " [ 70]\n",
            " [ 71]\n",
            " [ 56]\n",
            " [ 54]\n",
            " [ 64]\n",
            " [ 44]\n",
            " [ 61]\n",
            " [ 52]\n",
            " [ 56]\n",
            " [ 70]\n",
            " [ 69]\n",
            " [ 74]\n",
            " [ 73]\n",
            " [ 71]\n",
            " [ 82]\n",
            " [ 59]\n",
            " [ 50]\n",
            " [ 38]\n",
            " [ 81]\n",
            " [ 39]\n",
            " [ 68]\n",
            " [ 40]\n",
            " [ 80]\n",
            " [ 59]\n",
            " [ 78]\n",
            " [ 63]\n",
            " [ 71]\n",
            " [ 79]\n",
            " [ 42]\n",
            " [ 67]\n",
            " [ 76]\n",
            " [ 84]\n",
            " [ 74]\n",
            " [ 59]\n",
            " [ 57]\n",
            " [ 55]\n",
            " [ 55]\n",
            " [ 32]\n",
            " [ 95]\n",
            " [ 62]\n",
            " [ 72]\n",
            " [ 51]\n",
            " [ 46]\n",
            " [ 84]\n",
            " [ 68]\n",
            " [ 43]\n",
            " [ 73]\n",
            " [ 79]\n",
            " [ 81]\n",
            " [ 66]\n",
            " [ 93]\n",
            " [ 82]\n",
            " [ 78]\n",
            " [ 71]\n",
            " [ 67]\n",
            " [ 51]\n",
            " [ 89]\n",
            " [ 56]\n",
            " [ 67]\n",
            " [ 62]\n",
            " [ 72]\n",
            " [ 81]\n",
            " [ 67]\n",
            " [ 49]\n",
            " [ 64]\n",
            " [ 81]\n",
            " [ 99]\n",
            " [ 51]\n",
            " [ 78]\n",
            " [ 76]\n",
            " [ 64]\n",
            " [ 90]\n",
            " [ 67]\n",
            " [ 76]\n",
            " [ 43]\n",
            " [ 63]\n",
            " [ 82]\n",
            " [ 72]\n",
            " [ 69]\n",
            " [ 54]\n",
            " [ 54]\n",
            " [ 59]\n",
            " [ 66]\n",
            " [ 77]\n",
            " [ 70]\n",
            " [ 61]\n",
            " [ 75]\n",
            " [ 52]\n",
            " [ 77]\n",
            " [ 73]\n",
            " [ 55]\n",
            " [ 75]\n",
            " [ 45]\n",
            " [ 67]\n",
            " [ 57]\n",
            " [100]\n",
            " [ 67]\n",
            " [ 78]\n",
            " [ 56]\n",
            " [ 57]\n",
            " [ 83]\n",
            " [ 89]\n",
            " [100]\n",
            " [ 75]\n",
            " [ 60]\n",
            " [ 73]\n",
            " [ 81]\n",
            " [ 91]\n",
            " [ 61]\n",
            " [ 59]\n",
            " [ 65]\n",
            " [ 57]\n",
            " [ 57]\n",
            " [ 76]\n",
            " [ 77]\n",
            " [ 91]\n",
            " [ 68]\n",
            " [100]\n",
            " [ 57]\n",
            " [ 78]\n",
            " [ 80]\n",
            " [ 42]\n",
            " [ 66]\n",
            " [ 60]\n",
            " [ 67]\n",
            " [ 40]\n",
            " [ 64]\n",
            " [ 56]\n",
            " [ 61]\n",
            " [ 76]\n",
            " [ 90]\n",
            " [ 80]\n",
            " [ 72]\n",
            " [ 68]\n",
            " [ 40]\n",
            " [ 79]\n",
            " [ 77]\n",
            " [ 83]\n",
            " [ 96]\n",
            " [100]\n",
            " [ 72]\n",
            " [ 79]\n",
            " [ 80]\n",
            " [ 67]\n",
            " [ 53]\n",
            " [ 66]\n",
            " [ 91]\n",
            " [ 52]\n",
            " [ 97]\n",
            " [ 63]\n",
            " [ 69]\n",
            " [ 65]\n",
            " [ 65]\n",
            " [ 84]\n",
            " [ 79]\n",
            " [ 73]\n",
            " [ 92]\n",
            " [ 60]\n",
            " [ 84]\n",
            " [ 82]\n",
            " [ 44]\n",
            " [ 90]\n",
            " [ 69]\n",
            " [ 72]\n",
            " [ 69]\n",
            " [ 57]\n",
            " [ 93]\n",
            " [ 69]\n",
            " [ 75]\n",
            " [ 45]\n",
            " [ 46]\n",
            " [ 90]\n",
            " [ 66]\n",
            " [ 57]\n",
            " [ 65]\n",
            " [ 87]\n",
            " [ 39]\n",
            " [ 87]\n",
            " [ 77]\n",
            " [ 57]\n",
            " [ 59]\n",
            " [ 74]\n",
            " [ 60]\n",
            " [ 75]\n",
            " [ 95]\n",
            " [ 74]\n",
            " [ 53]\n",
            " [ 56]\n",
            " [ 91]\n",
            " [ 74]\n",
            " [ 47]\n",
            " [ 95]\n",
            " [ 49]\n",
            " [ 66]\n",
            " [100]\n",
            " [ 33]\n",
            " [ 99]\n",
            " [ 39]\n",
            " [ 56]\n",
            " [ 36]\n",
            " [ 58]\n",
            " [ 52]\n",
            " [ 59]\n",
            " [ 67]\n",
            " [ 55]\n",
            " [ 40]\n",
            " [ 32]\n",
            " [ 91]\n",
            " [ 31]\n",
            " [ 54]\n",
            " [ 69]\n",
            " [ 55]\n",
            " [ 73]\n",
            " [ 98]\n",
            " [ 60]\n",
            " [ 26]\n",
            " [ 95]\n",
            " [ 76]\n",
            " [ 53]\n",
            " [ 67]\n",
            " [ 80]\n",
            " [ 76]\n",
            " [ 74]\n",
            " [ 67]\n",
            " [ 81]\n",
            " [ 95]\n",
            " [ 58]\n",
            " [ 80]\n",
            " [ 83]\n",
            " [ 52]\n",
            " [ 69]\n",
            " [ 59]\n",
            " [ 62]\n",
            " [ 70]\n",
            " [ 65]\n",
            " [ 61]\n",
            " [ 42]\n",
            " [ 69]\n",
            " [ 76]\n",
            " [ 83]\n",
            " [ 62]\n",
            " [ 42]\n",
            " [ 64]\n",
            " [ 61]\n",
            " [ 69]\n",
            " [ 77]\n",
            " [ 80]\n",
            " [ 93]\n",
            " [ 63]\n",
            " [100]\n",
            " [ 88]\n",
            " [ 76]\n",
            " [ 72]\n",
            " [ 84]\n",
            " [ 86]\n",
            " [ 59]\n",
            " [ 77]\n",
            " [ 53]\n",
            " [ 87]\n",
            " [ 72]\n",
            " [ 67]\n",
            " [ 65]\n",
            " [ 84]\n",
            " [ 77]\n",
            " [ 77]\n",
            " [ 77]\n",
            " [ 45]\n",
            " [ 61]\n",
            " [ 61]\n",
            " [ 62]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 49]\n",
            " [ 13]\n",
            " [ 55]\n",
            " [ 85]\n",
            " [ 78]\n",
            " [ 69]\n",
            " [ 69]\n",
            " [ 45]\n",
            " [ 67]\n",
            " [ 68]\n",
            " [ 81]\n",
            " [ 73]\n",
            " [ 53]\n",
            " [ 80]\n",
            " [ 56]\n",
            " [ 82]\n",
            " [ 69]\n",
            " [ 76]\n",
            " [ 44]\n",
            " [ 25]\n",
            " [ 82]\n",
            " [ 55]\n",
            " [ 65]\n",
            " [ 79]\n",
            " [ 95]\n",
            " [ 62]\n",
            " [ 80]\n",
            " [ 93]\n",
            " [ 57]\n",
            " [ 54]\n",
            " [100]\n",
            " [ 67]\n",
            " [ 47]\n",
            " [ 69]\n",
            " [ 84]\n",
            " [ 50]\n",
            " [ 79]\n",
            " [ 50]\n",
            " [ 48]\n",
            " [ 56]\n",
            " [ 72]\n",
            " [ 47]\n",
            " [ 68]\n",
            " [ 79]\n",
            " [ 74]\n",
            " [ 37]\n",
            " [ 73]\n",
            " [ 92]\n",
            " [ 75]\n",
            " [ 61]\n",
            " [ 56]\n",
            " [ 52]\n",
            " [ 96]\n",
            " [ 59]\n",
            " [ 67]\n",
            " [ 89]\n",
            " [ 50]\n",
            " [ 51]\n",
            " [ 76]\n",
            " [ 80]\n",
            " [ 68]\n",
            " [ 46]\n",
            " [100]\n",
            " [ 44]\n",
            " [ 48]\n",
            " [ 67]\n",
            " [ 88]\n",
            " [ 74]\n",
            " [ 82]\n",
            " [ 77]\n",
            " [ 56]\n",
            " [ 96]\n",
            " [ 67]\n",
            " [ 60]\n",
            " [ 63]\n",
            " [ 77]\n",
            " [ 50]\n",
            " [ 44]\n",
            " [ 64]\n",
            " [ 50]\n",
            " [ 82]\n",
            " [ 91]\n",
            " [ 31]\n",
            " [ 67]\n",
            " [ 90]\n",
            " [ 89]\n",
            " [ 61]\n",
            " [ 64]\n",
            " [ 69]\n",
            " [ 78]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 64]\n",
            " [ 85]\n",
            " [ 68]\n",
            " [ 63]\n",
            " [ 68]\n",
            " [ 34]\n",
            " [ 77]\n",
            " [ 61]\n",
            " [ 53]\n",
            " [ 80]\n",
            " [ 62]\n",
            " [ 79]\n",
            " [ 83]\n",
            " [ 65]\n",
            " [ 58]\n",
            " [ 73]\n",
            " [ 94]\n",
            " [ 86]\n",
            " [ 76]\n",
            " [ 58]\n",
            " [ 61]\n",
            " [ 58]\n",
            " [ 64]\n",
            " [ 51]\n",
            " [ 82]\n",
            " [ 56]\n",
            " [ 65]\n",
            " [ 76]\n",
            " [ 48]\n",
            " [ 85]\n",
            " [ 68]\n",
            " [ 85]\n",
            " [ 62]\n",
            " [ 75]\n",
            " [ 38]\n",
            " [ 63]\n",
            " [ 68]\n",
            " [ 62]\n",
            " [ 73]\n",
            " [ 65]\n",
            " [ 77]\n",
            " [ 63]\n",
            " [ 83]\n",
            " [ 55]\n",
            " [ 87]\n",
            " [ 95]\n",
            " [ 48]\n",
            " [ 36]\n",
            " [ 84]\n",
            " [ 91]\n",
            " [ 79]\n",
            " [ 51]\n",
            " [ 76]\n",
            " [ 82]\n",
            " [ 71]\n",
            " [ 52]\n",
            " [ 60]\n",
            " [ 64]\n",
            " [ 68]\n",
            " [ 37]\n",
            " [ 67]\n",
            " [ 51]\n",
            " [ 78]\n",
            " [ 75]\n",
            " [ 60]\n",
            " [ 67]\n",
            " [ 51]\n",
            " [ 78]\n",
            " [ 64]\n",
            " [ 59]\n",
            " [ 85]\n",
            " [ 74]\n",
            " [ 40]\n",
            " [ 94]\n",
            " [ 58]\n",
            " [ 68]\n",
            " [ 83]\n",
            " [ 83]\n",
            " [ 68]\n",
            " [ 74]\n",
            " [ 85]\n",
            " [ 65]\n",
            " [ 68]\n",
            " [ 75]\n",
            " [ 52]\n",
            " [ 74]\n",
            " [ 60]\n",
            " [ 51]\n",
            " [ 64]\n",
            " [ 76]\n",
            " [ 45]\n",
            " [ 55]\n",
            " [ 45]\n",
            " [ 86]\n",
            " [ 85]\n",
            " [ 58]\n",
            " [ 72]\n",
            " [ 59]\n",
            " [ 60]\n",
            " [ 70]\n",
            " [ 79]\n",
            " [ 77]\n",
            " [ 77]\n",
            " [ 66]\n",
            " [ 68]\n",
            " [ 65]\n",
            " [ 70]\n",
            " [ 70]\n",
            " [ 55]\n",
            " [ 81]\n",
            " [ 62]\n",
            " [ 81]\n",
            " [ 71]\n",
            " [ 42]\n",
            " [ 62]\n",
            " [ 57]\n",
            " [ 80]\n",
            " [ 78]\n",
            " [ 71]\n",
            " [ 86]\n",
            " [ 76]\n",
            " [ 45]\n",
            " [ 71]\n",
            " [ 64]\n",
            " [ 69]\n",
            " [ 84]\n",
            " [ 65]\n",
            " [ 55]\n",
            " [ 64]\n",
            " [ 84]\n",
            " [ 52]\n",
            " [ 67]\n",
            " [ 77]\n",
            " [ 64]\n",
            " [ 58]\n",
            " [ 72]\n",
            " [ 73]\n",
            " [ 89]\n",
            " [ 83]\n",
            " [ 66]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the predicted Y values\n",
        "Y_pred = np.dot(W.T, X)\n",
        "print(\"Predicted Values (Y_pred):\")\n",
        "print(Y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOnz3O9k0OW2",
        "outputId": "4b79b85b-8445-4fc5-9b69-7843de6a2b9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Values (Y_pred):\n",
            "[[ 90.17733167 105.14250191 108.99193511 111.64074166  86.89466164\n",
            "  116.75790181 115.26346958  61.06130761 109.21864037 106.97699203\n",
            "   91.55841126 123.02943629  61.51471814 106.97699203  96.67557141\n",
            "   62.26193426 101.79273157  63.30295595 120.56108269 101.33932104\n",
            "   76.36653575  91.15125305 101.67937893  69.57449043 117.27841265\n",
            "  119.06665046  92.64568527  96.85602436 106.97699203 110.25966206\n",
            "  104.10148022 119.06665046 100.70545756  92.30562737  60.36034382\n",
            "   87.98193565  70.95557002 107.83756078  97.71659311  62.26193426\n",
            "  119.40670836 101.11261577  84.87971857  70.66176444  96.90227668\n",
            "  101.67937893  70.95557002 104.44153812 138.10795916 107.43040256\n",
            "  138.10795916 121.42165143 109.62579858  68.19341084  69.05397958\n",
            "   63.93681943  70.72886475  63.8234668   93.8000596  109.62579858\n",
            "   78.20102588 126.31210632 130.86250331 110.89352554  58.63920633\n",
            "  121.82880964  97.64949279 114.6296061   83.61199161  80.10261631\n",
            "   83.49863898  78.49483146  66.2918204   86.26079816  92.01182179\n",
            "   77.68051503  88.79625208  65.65795692 103.46761674  88.27574123\n",
            "  111.75409429 110.3730147  100.18494671 107.83756078  62.7824451\n",
            "   90.6307422  121.64835669 107.61085551 110.59971996 117.73182319\n",
            "   71.9294914   97.08272963 104.66824338 101.9060842   88.90960471\n",
            "   98.91721975  84.13250246  89.99687872 138.10795916 114.81005905\n",
            "   39.07738678  71.70278613 107.95091341  78.08767324  55.3565363\n",
            "  108.35807163  98.35045659  64.50358259 106.22977592  61.06130761\n",
            "   97.42278753 109.51244595 121.01449322 121.94216228  71.52233318\n",
            "  113.02182125  77.34045713 103.46761674 118.02562877 107.31704993\n",
            "   98.91721975 106.00307066 117.50511792  83.04522845  70.66176444\n",
            "   63.30295595  55.3565363   79.58210547  95.52119709 113.13517388\n",
            "  103.46761674 109.62579858  93.16619612  75.09880879  76.70659365\n",
            "  106.75028677 111.34693607 109.73915122 131.02210825 110.0329568\n",
            "   85.92074026 134.48523123 101.04551545  89.31676292  84.31295541\n",
            "  119.63341362  90.40403693 138.10795916 107.83756078  80.3964219\n",
            "   84.24585509 115.89733306 100.29829934  80.21596895  79.0153423\n",
            "   65.54460429 118.31943435 102.19988978  85.28687678 101.5660263\n",
            "  115.37682222  87.8223307   57.484832    97.37653521  89.9506264\n",
            "  104.55489075 117.27841265 101.45267367  98.91721975 128.96091287\n",
            "   84.47256035 101.97318451  86.14744553  82.11755938  99.21102533\n",
            "  123.32324187 112.1612525   87.86858302 103.98812759  87.52852512\n",
            "   93.39290138  77.86096798  95.74790235  87.00801427  94.43392308\n",
            "  116.87125444 107.72420815 118.65949225 113.36187915 113.65568473\n",
            "   81.59704854 123.66329977  81.14363801  95.40784445  75.95937754\n",
            "  107.31704993 116.41784391  87.93568333 106.45648119 104.32818548\n",
            "  126.01830073 101.04551545  91.37795831  95.52119709 118.02562877\n",
            "   65.43125165  80.73647979  66.06511513 109.6720509   85.17352415\n",
            "   78.72153672  81.48369591 105.0754016   82.57096992  93.27954875\n",
            "  134.19142565  65.02409344 127.57983328  81.89085412 113.02182125\n",
            "   67.96670557 102.94710589  84.01914982 114.44915315 113.88238999\n",
            "  109.10528774  92.41898001  97.08272963  90.51738957  92.48608032\n",
            "   84.17875477  86.37415079 101.97318451 102.72040063 108.87858247\n",
            "  118.54613961 113.99574262  58.63920633  98.80386712 107.49750288\n",
            "  104.32818548  50.0589232  100.93216282  62.55573984  88.27574123\n",
            "  107.27079762  69.28068485  44.0811943   88.5024465   61.51471814\n",
            "  124.59096883  86.37415079  71.70278613 115.15011695 119.40670836\n",
            "  107.49750288  46.16323769 100.47875229 105.93597034  89.24966261\n",
            "   86.78130901  86.89466164  66.17846777  24.74608002  89.13630998\n",
            "  110.0329568  106.75028677  94.32057044  89.72392114  85.51358205\n",
            "  130.34199246  60.3140915   91.78511653  60.02028592 103.76142232\n",
            "   85.85363995  78.49483146 114.969664    91.03790041  95.40784445\n",
            "   93.57335433  75.09880879  68.94062695  90.6307422  104.32818548\n",
            "  126.13165337 103.694322    63.75636648  81.59704854  80.16971663\n",
            "   93.39290138 107.09034467  88.1623886   93.05284349  72.33664961\n",
            "   73.08386572  66.06511513  58.00534285 106.97699203  79.98926368\n",
            "  134.30477828 120.44773005  90.74409483  65.02409344  96.90227668\n",
            "   72.85716046  96.2684132   69.62074274 104.50863843 111.41403639\n",
            "   77.97432061 128.55375465  88.72915176 115.78398043 104.84869633\n",
            "  107.31704993  91.73886421 103.98812759  99.03057238  87.52852512\n",
            "   73.83108184  92.12517442 100.41165197  88.56954681  83.90579719\n",
            "  101.2259684  116.98460707 101.5660263  113.1814262  104.62199107\n",
            "  111.52738902 101.15886809  96.49511846  76.59324102 131.08920857\n",
            "   95.52119709 101.79273157  82.75142286 126.31210632  78.08767324\n",
            "  112.72801567  82.11755938 127.80653854  92.82613822  82.00420675\n",
            "  107.83756078 133.21750427 101.79273157  88.38909386 101.11261577\n",
            "  110.48636733  44.30789956 132.47028816  87.75523038 125.79159547\n",
            "   46.95670611 100.70545756 114.40290084  80.62312716 109.10528774\n",
            "  116.64454918 113.24852651  74.57829795  59.6131277   77.11375187\n",
            "   84.9930712   67.03903651 102.19988978  86.55460374  90.81119515\n",
            "  117.16506002  88.1623886   97.53614016 102.60704799  92.93949085\n",
            "   87.86858302  81.55079622 103.92102727 113.88238999 116.75790181\n",
            "  109.17238805 112.09415219  98.056651   113.99574262 102.53994768\n",
            "   90.51738957 102.31324241  74.98545616  95.18113919 116.0106857\n",
            "   88.72915176 100.47875229  59.15971717 120.607335    69.23443253\n",
            "   79.46875283  97.60324047 100.47875229 117.7989235   90.2906843\n",
            "  138.10795916  77.81471566 105.0754016   83.15858108  91.89846916\n",
            "  111.12023081 102.83375326  60.42744413  93.16619612  83.38528634\n",
            "  122.57602576 124.07045798  84.42630804 110.48636733  82.63807023\n",
            "   73.7177292   49.15210214 113.47523178 133.33085691  99.32437796\n",
            "   88.1623886   84.53966067 101.85983188  60.42744413  89.24966261\n",
            "  136.8402322   71.70278613  52.14096658 113.76903736 126.01830073\n",
            "   99.4377306   53.11488796 106.90989172 109.10528774  77.2271045\n",
            "  103.58096937 122.39557281 132.47028816  92.93949085  53.86210407\n",
            "  119.29335573 116.87125444  73.31057099 129.70812898  98.46380922\n",
            "  113.47523178 109.62579858  80.10261631  97.01562931  82.23091202\n",
            "   65.83840987  91.89846916  97.19608226  79.12869494  73.24347067\n",
            "  119.63341362  60.54079677 114.06284294 110.30591438  92.01182179\n",
            "   89.24966261  71.70278613  62.55573984  58.1857958   92.12517442\n",
            "  121.76170933  99.55108323  98.23710395  83.79244456 116.93835476\n",
            "   88.90960471 107.31704993 110.25966206  86.60085606 102.83375326\n",
            "  113.65568473  85.40022942  77.79386766  86.37415079  83.31818603\n",
            "  105.14250191  75.50596701  98.056651    91.44505863 111.34693607\n",
            "  106.22977592  59.56687539 109.73915122  68.19341084  97.30943489\n",
            "  118.59239193 101.45267367  80.10261631 137.47409568 113.02182125\n",
            "  107.38415025  96.90227668 115.44392253  71.18227528  76.70659365\n",
            "   92.93949085  57.07767379  81.48369591  87.41517249  71.88323908\n",
            "   88.1623886  107.02324435 123.66329977 106.8636394  100.70545756\n",
            "   94.66062834  77.45380976  77.97432061  61.58181846 107.83756078\n",
            "   62.48863952  80.62312716  64.91074081 110.89352554  67.21948946\n",
            "  107.27079762  95.29449182  80.84983243 111.05313049  67.21948946\n",
            "   77.74761534 102.60704799 102.19988978 100.41165197  75.32551406\n",
            "   66.40517303  78.90198967 116.53119654  34.18693189 138.10795916\n",
            "   94.43392308 118.43278698  69.86829601  61.92187636 107.27079762\n",
            "   85.21977647  53.40869354  88.72915176 119.52006099 106.34312855\n",
            "  126.9459698  103.46761674 134.48523123 113.88238999 110.25966206\n",
            "  100.81881019  75.61931964 126.9459698   93.05284349  93.8000596\n",
            "   92.35187969 104.10148022  97.42278753  85.51358205  63.64301385\n",
            "  101.79273157 112.95472093 126.53881158  55.99039978 109.96585648\n",
            "  117.50511792  75.73267227 120.607335    90.74409483 135.11909471\n",
            "   70.09500127  94.88733361  97.24233458 100.07159408 101.2259684\n",
            "   89.43011556  93.16619612  75.09880879  99.03057238  89.54346819\n",
            "  117.68557087  81.25699064  94.02676486  74.98545616 121.87506196\n",
            "   98.91721975  99.55108323 104.21483285  53.52204618  96.15506057\n",
            "   85.62693468 138.10795916  69.68784306  96.49511846  85.40022942\n",
            "   97.24233458 113.99574262 119.06665046 136.8402322   91.26460568\n",
            "   99.95824144 111.41403639  95.06778656 130.56869772  78.78863704\n",
            "   98.69051448  98.3967089   85.40022942  91.89846916 102.76665294\n",
            "  106.11642329 130.56869772  98.46380922 107.31704993  68.19341084\n",
            "   89.9506264  113.24852651  51.96051363  98.57716185  97.30943489\n",
            "   96.56221878  44.19454693 109.73915122  73.19721836  89.09005766\n",
            "  112.09415219 137.47409568 110.48636733 112.38795777 116.41784391\n",
            "   76.70659365 114.6296061  117.39176529 127.23977538 114.40290084\n",
            "  132.47028816  90.81119515  97.94329837 120.38062974  84.01914982\n",
            "   75.43886669  85.62693468 132.47028816  88.90960471 110.71307259\n",
            "   95.9283553   85.17352415 106.00307066  93.68670696 112.61466303\n",
            "  127.91989117  96.67557141 110.25966206  83.83869687 130.11528719\n",
            "  100.47875229  85.62693468 108.47142426  87.23471954 106.63693414\n",
            "   79.24204757  89.77017346 138.10795916  96.67557141  96.85602436\n",
            "   71.40898055 100.07159408 125.5648902   77.97432061  89.13630998\n",
            "   68.4201161  134.3718786   53.11488796 110.66682028 112.8413683\n",
            "   96.72182373  79.12869494 109.15154005  96.67557141 102.72040063\n",
            "  111.12023081 115.26346958  73.19721836  96.33551352 138.10795916\n",
            "   93.8000596   66.47227335 128.14659644  65.65795692 108.87858247\n",
            "  138.10795916  57.77863758 124.41051588  79.12869494  56.62426326\n",
            "   31.99153587  88.27574123  81.48369591  88.27574123 108.87858247\n",
            "   66.06511513  68.94062695  61.62807078 127.80653854  55.12983103\n",
            "   82.63807023 120.15392447  81.25699064 106.97699203 136.8402322\n",
            "   89.13630998  52.66147743 121.42165143 113.76903736  85.74028731\n",
            "   97.82994574 104.7353437   96.67557141 124.63722114  88.90960471\n",
            "  109.331993   115.03676432  85.74028731 117.61847055 111.30068376\n",
            "   79.0153423  126.71926453  88.27574123 105.59591244 104.66824338\n",
            "  106.11642329  72.22329698  54.15590965 118.95329783 116.53119654\n",
            "  120.56108269 110.07920911  54.08880934 110.07920911  67.85335294\n",
            "   81.89085412 117.09795971 113.76903736 136.61352694  85.51358205\n",
            "  132.47028816 114.10909526  95.52119709  74.35159268  99.95824144\n",
            "  126.19875368 101.67937893  88.79625208  88.27574123 115.60352748\n",
            "   98.57716185 104.44153812  94.54727571 111.75409429  88.09528828\n",
            "   88.72915176 122.91608365  80.32932158 107.83756078 109.85250385\n",
            "  107.72420815  95.40784445 101.97318451  65.09119376  22.50443168\n",
            "   79.0153423  127.69318591  99.95824144  92.30562737  82.00420675\n",
            "   83.04522845  96.969377    86.48750342 109.96585648  86.73505669\n",
            "   62.7824451  121.3082988   77.52091008 102.94710589 115.03676432\n",
            "   84.35920772  61.80852373  35.9751697  100.70545756  77.2271045\n",
            "   90.51738957  97.19608226 118.77284488  93.16619612 109.62579858\n",
            "  115.49017485  93.68670696  67.44619472 138.10795916  87.00801427\n",
            "   86.26079816  84.24585509  95.9283553   79.3554002  108.13136636\n",
            "   71.81613876  71.88323908  75.50596701  86.14744553  64.05017206\n",
            "   89.09005766 115.6706278   97.37653521  77.00039923  82.34426465\n",
            "  121.12784585 106.00307066  84.35920772  94.43392308  72.85716046\n",
            "  119.18000309  99.84488881  76.8870466  130.86250331  93.57335433\n",
            "   91.03790041  95.70165004 103.87477495  87.98193565  83.72534424\n",
            "  137.36074305  84.9930712   75.73267227  86.32789848 110.89352554\n",
            "   78.60818409 119.52006099 109.73915122  88.27574123 119.52006099\n",
            "  116.87125444 103.58096937  95.40784445  91.73886421  68.30676347\n",
            "   59.79358065 104.55489075  76.29943544 105.48255981 121.64835669\n",
            "   50.75988699  82.23091202 111.75409429 122.46267312 105.88971802\n",
            "   83.90579719  95.29449182 106.00307066  73.08386572  81.37034327\n",
            "  107.2036973  114.969664    86.14744553  93.91341223  93.8000596\n",
            "   65.02409344  92.93949085  95.74790235  79.3554002  102.08653715\n",
            "   79.98926368 107.31704993 114.85631137 107.43040256  83.61199161\n",
            "   97.82994574 138.10795916 132.92369869 125.15773199 100.07159408\n",
            "   82.5247176   60.13363855  77.68051503  90.06397904 124.00335766\n",
            "   89.02295734  85.06017152 100.70545756  59.04636454 120.56108269\n",
            "   81.59704854 113.99574262  92.07892211  99.03057238  62.66909247\n",
            "   86.78130901  97.76284542  95.81500267 110.25966206  72.85716046\n",
            "   99.21102533  98.35045659 115.15011695  66.69897861 133.21750427\n",
            "  136.8402322   60.13363855  61.69517109 129.93483424 107.83756078\n",
            "   94.43392308  75.61931964 120.15392447 105.48255981  86.78130901\n",
            "   55.46988893  84.76636594  81.89085412  90.92454778  72.45000224\n",
            "   92.01182179  83.90579719 123.36949418 106.11642329 110.78017291\n",
            "   97.48988784  84.47256035 121.12784585  61.92187636  91.03790041\n",
            "  130.56869772 115.60352748  46.61664822 130.56869772  68.64682137\n",
            "   94.61437603 116.35074359  98.57716185  81.48369591 116.0106857\n",
            "  111.00687818 102.53994768 101.67937893  91.15125305  96.56221878\n",
            "  119.92721921  85.10642383  65.2507987   78.20102588 114.92341169\n",
            "   62.37528689  77.2271045   87.41517249 123.95710535 112.04789987\n",
            "   79.64920578  89.88352609  93.27954875  75.84602491 100.99926314\n",
            "  104.10148022 113.13517388  98.17000364  87.41517249 101.15886809\n",
            "   89.54346819  90.2906843  100.18494671  88.1623886  127.69318591\n",
            "   95.9283553   97.94329837  98.01039869  76.36653575 100.93216282\n",
            "   86.78130901 111.12023081 114.85631137  88.79625208 112.38795777\n",
            "  115.89733306  76.07273017 102.42659504  76.8870466   76.77369397\n",
            "  102.83375326  84.42630804  87.64187775  86.14744553 123.32324187\n",
            "   82.23091202  72.04284403 128.62085497  90.6307422   74.80500321\n",
            "   99.21102533 121.76170933 125.38443726 110.25966206  95.63454972]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To do Task 3"
      ],
      "metadata": {
        "id": "GPyxf3CP0YUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming the data is already loaded in `df` as:\n",
        "# df = pd.DataFrame({'Math': [48, 62, 79, 76, 59], 'Reading': [68, 81, 80, 83, 64], 'Writing': [63, 72, 78, 79, 62]})\n",
        "\n",
        "# Features (X) and Target (Y)\n",
        "X = df[['Reading', 'Writing']].values  # Features: Reading and Writing\n",
        "Y = df['Math'].values.reshape(-1, 1)  # Target: Math scores\n",
        "\n",
        "# Step 1: Split the dataset into training and test sets (80% training, 20% testing)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the splits to confirm\n",
        "print(\"Training Features (X_train) shape:\", X_train.shape)\n",
        "print(\"Testing Features (X_test) shape:\", X_test.shape)\n",
        "print(\"Training Labels (Y_train) shape:\", Y_train.shape)\n",
        "print(\"Testing Labels (Y_test) shape:\", Y_test.shape)\n",
        "\n",
        "# Step 2: Create the Weight Vector W (d x 1) for linear regression\n",
        "d = X.shape[1]  # Number of features (Reading and Writing)\n",
        "W = np.random.rand(d, 1)  # Initialize W with random values\n",
        "\n",
        "# Step 3: Compute the predicted Y values (Y_pred) using W^T * X\n",
        "Y_pred_train = np.dot(X_train, W)  # Prediction for training data\n",
        "Y_pred_test = np.dot(X_test, W)  # Prediction for test data\n",
        "\n",
        "# Print the matrices and predictions\n",
        "print(\"\\nFeature Matrix X (Training):\")\n",
        "print(X_train)\n",
        "print(\"Feature Matrix X (Testing):\")\n",
        "print(X_test)\n",
        "\n",
        "print(\"\\nWeight Vector W:\")\n",
        "print(W)\n",
        "\n",
        "print(\"\\nPredicted Y values (Training):\")\n",
        "print(Y_pred_train)\n",
        "\n",
        "print(\"\\nPredicted Y values (Testing):\")\n",
        "print(Y_pred_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCPtrpVn0cJq",
        "outputId": "fbb1eec8-8eff-45de-8f54-71cb209a1658"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features (X_train) shape: (800, 2)\n",
            "Testing Features (X_test) shape: (200, 2)\n",
            "Training Labels (Y_train) shape: (800, 1)\n",
            "Testing Labels (Y_test) shape: (200, 1)\n",
            "\n",
            "Feature Matrix X (Training):\n",
            "[[82 78]\n",
            " [70 67]\n",
            " [21 25]\n",
            " ...\n",
            " [76 79]\n",
            " [75 75]\n",
            " [76 80]]\n",
            "Feature Matrix X (Testing):\n",
            "[[ 69  69]\n",
            " [ 37  41]\n",
            " [ 62  57]\n",
            " [ 59  56]\n",
            " [ 92  88]\n",
            " [ 70  70]\n",
            " [ 99 100]\n",
            " [ 50  40]\n",
            " [ 60  58]\n",
            " [ 79  82]\n",
            " [ 87  81]\n",
            " [ 67  62]\n",
            " [ 69  63]\n",
            " [ 58  53]\n",
            " [ 59  59]\n",
            " [ 56  50]\n",
            " [ 71  65]\n",
            " [ 69  62]\n",
            " [ 89  89]\n",
            " [ 65  62]\n",
            " [ 82  78]\n",
            " [ 74  70]\n",
            " [ 66  62]\n",
            " [ 74  74]\n",
            " [ 57  57]\n",
            " [ 88  84]\n",
            " [ 53  51]\n",
            " [ 66  57]\n",
            " [ 85  82]\n",
            " [ 44  50]\n",
            " [ 97  96]\n",
            " [ 84  76]\n",
            " [ 32  32]\n",
            " [ 55  56]\n",
            " [ 87  83]\n",
            " [ 76  78]\n",
            " [ 89  96]\n",
            " [ 85  91]\n",
            " [ 75  68]\n",
            " [ 81  78]\n",
            " [ 76  87]\n",
            " [ 62  60]\n",
            " [ 61  67]\n",
            " [ 72  74]\n",
            " [ 59  57]\n",
            " [ 80  86]\n",
            " [ 84  77]\n",
            " [ 85  77]\n",
            " [ 39  39]\n",
            " [ 80  79]\n",
            " [ 54  48]\n",
            " [ 73  72]\n",
            " [ 64  62]\n",
            " [ 69  74]\n",
            " [ 52  49]\n",
            " [ 77  74]\n",
            " [ 36  39]\n",
            " [ 69  64]\n",
            " [ 74  75]\n",
            " [ 89  84]\n",
            " [ 50  46]\n",
            " [ 50  50]\n",
            " [ 61  67]\n",
            " [ 66  58]\n",
            " [ 63  52]\n",
            " [ 89  91]\n",
            " [100 100]\n",
            " [ 70  65]\n",
            " [ 61  60]\n",
            " [ 86  87]\n",
            " [ 44  49]\n",
            " [ 99 100]\n",
            " [ 62  62]\n",
            " [ 81  88]\n",
            " [ 66  62]\n",
            " [ 64  53]\n",
            " [ 82  78]\n",
            " [ 44  46]\n",
            " [ 68  71]\n",
            " [ 45  55]\n",
            " [ 76  73]\n",
            " [ 68  73]\n",
            " [ 60  64]\n",
            " [ 83  83]\n",
            " [ 69  74]\n",
            " [ 83  82]\n",
            " [ 82  78]\n",
            " [ 61  63]\n",
            " [ 65  63]\n",
            " [ 73  74]\n",
            " [ 71  71]\n",
            " [ 66  67]\n",
            " [ 58  59]\n",
            " [ 62  57]\n",
            " [ 32  21]\n",
            " [ 87  90]\n",
            " [ 79  80]\n",
            " [ 79  81]\n",
            " [ 83  78]\n",
            " [ 72  67]\n",
            " [ 78  74]\n",
            " [ 31  33]\n",
            " [ 81  87]\n",
            " [ 41  27]\n",
            " [ 97  95]\n",
            " [ 78  82]\n",
            " [ 69  67]\n",
            " [ 78  69]\n",
            " [ 70  73]\n",
            " [ 62  57]\n",
            " [ 70  75]\n",
            " [ 98  92]\n",
            " [ 75  74]\n",
            " [ 80  74]\n",
            " [ 87  89]\n",
            " [ 72  82]\n",
            " [ 70  67]\n",
            " [ 67  62]\n",
            " [100 100]\n",
            " [ 54  51]\n",
            " [ 71  63]\n",
            " [ 98 100]\n",
            " [ 84  81]\n",
            " [ 65  61]\n",
            " [ 43  42]\n",
            " [ 45  47]\n",
            " [ 57  52]\n",
            " [ 80  84]\n",
            " [ 65  73]\n",
            " [ 63  62]\n",
            " [ 65  67]\n",
            " [ 87  89]\n",
            " [ 84  76]\n",
            " [ 74  70]\n",
            " [ 54  55]\n",
            " [ 49  44]\n",
            " [ 70  73]\n",
            " [ 95  99]\n",
            " [ 90  83]\n",
            " [ 48  48]\n",
            " [ 68  72]\n",
            " [ 33  31]\n",
            " [ 86  81]\n",
            " [ 56  53]\n",
            " [ 69  74]\n",
            " [ 65  61]\n",
            " [ 71  59]\n",
            " [ 65  63]\n",
            " [ 60  52]\n",
            " [ 64  69]\n",
            " [ 66  63]\n",
            " [ 65  59]\n",
            " [ 81  85]\n",
            " [ 88  94]\n",
            " [ 71  77]\n",
            " [ 48  43]\n",
            " [ 85  77]\n",
            " [ 59  59]\n",
            " [ 55  58]\n",
            " [ 86  93]\n",
            " [ 65  75]\n",
            " [ 76  72]\n",
            " [ 81  82]\n",
            " [ 91  94]\n",
            " [ 46  48]\n",
            " [ 57  56]\n",
            " [ 78  77]\n",
            " [ 81  77]\n",
            " [ 77  75]\n",
            " [ 57  56]\n",
            " [ 82  82]\n",
            " [ 56  64]\n",
            " [ 70  71]\n",
            " [ 92  87]\n",
            " [ 48  39]\n",
            " [ 41  45]\n",
            " [ 87  86]\n",
            " [ 63  66]\n",
            " [ 70  69]\n",
            " [ 72  85]\n",
            " [ 84  80]\n",
            " [ 49  50]\n",
            " [ 72  78]\n",
            " [ 77  72]\n",
            " [ 46  41]\n",
            " [ 80  79]\n",
            " [ 34  34]\n",
            " [ 61  62]\n",
            " [ 72  75]\n",
            " [ 74  71]\n",
            " [ 55  58]\n",
            " [ 59  62]\n",
            " [ 62  62]\n",
            " [ 75  76]\n",
            " [ 56  63]\n",
            " [ 68  67]\n",
            " [ 65  74]\n",
            " [ 74  84]\n",
            " [ 75  80]\n",
            " [ 76  74]]\n",
            "\n",
            "Weight Vector W:\n",
            "[[0.0538367 ]\n",
            " [0.62836428]]\n",
            "\n",
            "Predicted Y values (Training):\n",
            "[[53.42702333]\n",
            " [45.86897584]\n",
            " [16.83967774]\n",
            " [48.38243297]\n",
            " [59.29764867]\n",
            " [56.56884475]\n",
            " [52.94249305]\n",
            " [43.87620959]\n",
            " [51.48809378]\n",
            " [44.66608397]\n",
            " [58.36394029]\n",
            " [65.65280429]\n",
            " [45.56363174]\n",
            " [49.49532753]\n",
            " [48.07708887]\n",
            " [51.52425439]\n",
            " [45.50979504]\n",
            " [30.64520746]\n",
            " [28.40093381]\n",
            " [52.58331225]\n",
            " [34.43306925]\n",
            " [56.78419154]\n",
            " [41.82960665]\n",
            " [54.8814226 ]\n",
            " [50.10601573]\n",
            " [53.31934993]\n",
            " [52.52947556]\n",
            " [54.5760785 ]\n",
            " [65.5451309 ]\n",
            " [56.94570163]\n",
            " [39.72916701]\n",
            " [37.98790816]\n",
            " [35.22294362]\n",
            " [42.61948102]\n",
            " [44.55841057]\n",
            " [56.99953833]\n",
            " [42.92482512]\n",
            " [41.88344334]\n",
            " [38.52627514]\n",
            " [37.57489067]\n",
            " [54.10922431]\n",
            " [58.0409201 ]\n",
            " [53.78620412]\n",
            " [42.97866182]\n",
            " [48.84928716]\n",
            " [28.34709711]\n",
            " [49.11847065]\n",
            " [32.6918104 ]\n",
            " [30.75288085]\n",
            " [51.73960118]\n",
            " [49.74683493]\n",
            " [36.6235062 ]\n",
            " [37.62872736]\n",
            " [30.17835327]\n",
            " [56.89186493]\n",
            " [41.41658915]\n",
            " [31.12973774]\n",
            " [41.09356897]\n",
            " [63.01399767]\n",
            " [57.93324671]\n",
            " [34.37923255]\n",
            " [61.18274152]\n",
            " [45.49211895]\n",
            " [51.43425708]\n",
            " [55.47362627]\n",
            " [38.93929263]\n",
            " [68.22009812]\n",
            " [51.07507629]\n",
            " [56.62268144]\n",
            " [56.40733465]\n",
            " [60.91355803]\n",
            " [58.0409201 ]\n",
            " [47.96941548]\n",
            " [59.72834225]\n",
            " [57.44871643]\n",
            " [52.58331225]\n",
            " [56.20966395]\n",
            " [64.75525652]\n",
            " [54.00155091]\n",
            " [51.1112369 ]\n",
            " [51.21891029]\n",
            " [32.27879291]\n",
            " [52.06262137]\n",
            " [46.91035761]\n",
            " [40.24985789]\n",
            " [47.75406869]\n",
            " [53.89387752]\n",
            " [40.87822218]\n",
            " [46.65885021]\n",
            " [31.84809933]\n",
            " [42.81715173]\n",
            " [45.24061155]\n",
            " [30.26835057]\n",
            " [46.80268422]\n",
            " [36.83885299]\n",
            " [39.31614951]\n",
            " [43.62470219]\n",
            " [55.41978957]\n",
            " [28.34709711]\n",
            " [32.11728282]\n",
            " [24.30772792]\n",
            " [58.1485935 ]\n",
            " [48.43626967]\n",
            " [48.75928985]\n",
            " [55.09676939]\n",
            " [54.95293538]\n",
            " [64.23456563]\n",
            " [45.76130244]\n",
            " [35.20526754]\n",
            " [61.64959571]\n",
            " [41.14740567]\n",
            " [33.32017469]\n",
            " [39.10080272]\n",
            " [39.10080272]\n",
            " [49.47765144]\n",
            " [37.79023746]\n",
            " [44.97142806]\n",
            " [64.91676661]\n",
            " [51.90111127]\n",
            " [52.42180216]\n",
            " [43.08633521]\n",
            " [53.78620412]\n",
            " [46.12048324]\n",
            " [54.73758859]\n",
            " [43.49935271]\n",
            " [55.97664107]\n",
            " [63.39085456]\n",
            " [51.95494797]\n",
            " [23.67936364]\n",
            " [36.3181621 ]\n",
            " [63.39085456]\n",
            " [64.01921884]\n",
            " [42.87098842]\n",
            " [56.20966395]\n",
            " [43.24784531]\n",
            " [41.68577264]\n",
            " [59.35148537]\n",
            " [65.65280429]\n",
            " [57.77173661]\n",
            " [56.78419154]\n",
            " [40.50136529]\n",
            " [45.65362905]\n",
            " [41.77576995]\n",
            " [30.23218997]\n",
            " [31.86577542]\n",
            " [42.51180763]\n",
            " [62.18796269]\n",
            " [31.65042863]\n",
            " [36.04897861]\n",
            " [57.30488242]\n",
            " [37.68256406]\n",
            " [48.92079995]\n",
            " [41.82960665]\n",
            " [39.56765691]\n",
            " [53.15783984]\n",
            " [53.37318663]\n",
            " [67.89707794]\n",
            " [44.86375467]\n",
            " [59.08230188]\n",
            " [38.79545862]\n",
            " [28.29326042]\n",
            " [56.31733735]\n",
            " [51.43425708]\n",
            " [30.44753676]\n",
            " [42.09879013]\n",
            " [52.58331225]\n",
            " [44.55841057]\n",
            " [47.64639529]\n",
            " [46.49734012]\n",
            " [54.68375189]\n",
            " [62.92400037]\n",
            " [39.7830037 ]\n",
            " [40.62671478]\n",
            " [40.30369459]\n",
            " [56.67651814]\n",
            " [64.91676661]\n",
            " [42.92482512]\n",
            " [42.35029753]\n",
            " [45.86897584]\n",
            " [36.47967219]\n",
            " [49.06463395]\n",
            " [59.87217626]\n",
            " [58.66928439]\n",
            " [52.74482235]\n",
            " [31.75810202]\n",
            " [51.38042038]\n",
            " [44.61224727]\n",
            " [45.76130244]\n",
            " [40.87822218]\n",
            " [39.72916701]\n",
            " [58.50777429]\n",
            " [48.13092557]\n",
            " [43.08633521]\n",
            " [51.95494797]\n",
            " [24.88225551]\n",
            " [33.12250398]\n",
            " [54.6299152 ]\n",
            " [36.73117959]\n",
            " [46.44350342]\n",
            " [63.75003535]\n",
            " [57.41255582]\n",
            " [46.19199602]\n",
            " [50.91356619]\n",
            " [36.64118228]\n",
            " [47.91557878]\n",
            " [27.97024023]\n",
            " [42.76331503]\n",
            " [62.13412599]\n",
            " [59.45915876]\n",
            " [61.34425161]\n",
            " [39.04696602]\n",
            " [46.22815663]\n",
            " [58.88463118]\n",
            " [44.28922708]\n",
            " [42.18878744]\n",
            " [44.28922708]\n",
            " [36.15665201]\n",
            " [36.3719988 ]\n",
            " [53.48086003]\n",
            " [39.56765691]\n",
            " [58.25626689]\n",
            " [47.2872145 ]\n",
            " [43.3555187 ]\n",
            " [46.12048324]\n",
            " [53.89387752]\n",
            " [50.48287261]\n",
            " [53.78620412]\n",
            " [62.13412599]\n",
            " [60.50054054]\n",
            " [45.07910146]\n",
            " [64.37839964]\n",
            " [57.35871912]\n",
            " [62.65481688]\n",
            " [28.867788  ]\n",
            " [44.45073718]\n",
            " [30.64520746]\n",
            " [49.36997805]\n",
            " [40.68055147]\n",
            " [29.96300648]\n",
            " [42.09879013]\n",
            " [58.86695509]\n",
            " [50.62670662]\n",
            " [47.1257044 ]\n",
            " [42.70947833]\n",
            " [62.18796269]\n",
            " [46.96419431]\n",
            " [60.51821663]\n",
            " [53.58853342]\n",
            " [36.5696695 ]\n",
            " [52.33180485]\n",
            " [49.63916154]\n",
            " [54.8814226 ]\n",
            " [61.64959571]\n",
            " [44.98910415]\n",
            " [44.23539039]\n",
            " [55.47362627]\n",
            " [43.7146995 ]\n",
            " [44.28922708]\n",
            " [44.18155369]\n",
            " [39.36998621]\n",
            " [46.22815663]\n",
            " [58.93846788]\n",
            " [68.11242473]\n",
            " [55.36595288]\n",
            " [45.38444556]\n",
            " [27.86256684]\n",
            " [37.14419708]\n",
            " [57.08953563]\n",
            " [37.10803648]\n",
            " [53.75004351]\n",
            " [40.41136799]\n",
            " [53.51702063]\n",
            " [64.75525652]\n",
            " [59.71066616]\n",
            " [51.57809109]\n",
            " [41.77576995]\n",
            " [39.8906771 ]\n",
            " [64.75525652]\n",
            " [48.49010637]\n",
            " [49.99834233]\n",
            " [64.75525652]\n",
            " [43.14017191]\n",
            " [61.29041492]\n",
            " [46.13815932]\n",
            " [15.33144177]\n",
            " [35.94130522]\n",
            " [41.66809655]\n",
            " [50.12369182]\n",
            " [49.85450833]\n",
            " [54.43224449]\n",
            " [68.22009812]\n",
            " [56.99953833]\n",
            " [52.36796546]\n",
            " [18.006409  ]\n",
            " [34.89992344]\n",
            " [50.06985512]\n",
            " [47.34105119]\n",
            " [21.6865974 ]\n",
            " [38.99312933]\n",
            " [39.92683771]\n",
            " [54.6299152 ]\n",
            " [44.18155369]\n",
            " [39.51382021]\n",
            " [35.79747121]\n",
            " [45.76130244]\n",
            " [50.6443827 ]\n",
            " [68.22009812]\n",
            " [56.56884475]\n",
            " [44.91759137]\n",
            " [51.0035635 ]\n",
            " [48.07708887]\n",
            " [29.54998898]\n",
            " [48.22092288]\n",
            " [31.27357174]\n",
            " [39.8906771 ]\n",
            " [54.68375189]\n",
            " [40.46520468]\n",
            " [42.45797093]\n",
            " [35.90514461]\n",
            " [51.0035635 ]\n",
            " [49.22614404]\n",
            " [49.17230735]\n",
            " [58.3462642 ]\n",
            " [56.46117135]\n",
            " [25.97747398]\n",
            " [35.79747121]\n",
            " [39.5314963 ]\n",
            " [48.22092288]\n",
            " [38.14941825]\n",
            " [52.15261867]\n",
            " [41.72193325]\n",
            " [41.56042316]\n",
            " [53.94771422]\n",
            " [51.79343788]\n",
            " [33.48168478]\n",
            " [37.62872736]\n",
            " [40.1421845 ]\n",
            " [39.92683771]\n",
            " [51.38042038]\n",
            " [53.37318663]\n",
            " [56.51500805]\n",
            " [27.71873283]\n",
            " [56.56884475]\n",
            " [53.94771422]\n",
            " [50.59054601]\n",
            " [45.02526476]\n",
            " [38.25709165]\n",
            " [44.45073718]\n",
            " [68.22009812]\n",
            " [27.86256684]\n",
            " [38.20325495]\n",
            " [44.07388029]\n",
            " [37.79023746]\n",
            " [29.71149908]\n",
            " [46.38966672]\n",
            " [59.35148537]\n",
            " [43.301682  ]\n",
            " [40.94973496]\n",
            " [34.27155916]\n",
            " [48.13092557]\n",
            " [54.30689501]\n",
            " [48.79545046]\n",
            " [53.48086003]\n",
            " [55.67129697]\n",
            " [11.70509008]\n",
            " [63.55236465]\n",
            " [51.1473975 ]\n",
            " [48.90312386]\n",
            " [40.30369459]\n",
            " [43.82237289]\n",
            " [37.89791085]\n",
            " [35.94130522]\n",
            " [28.9754614 ]\n",
            " [68.22009812]\n",
            " [43.6070261 ]\n",
            " [61.12890482]\n",
            " [50.9497268 ]\n",
            " [43.14017191]\n",
            " [43.82237289]\n",
            " [56.40733465]\n",
            " [36.2104887 ]\n",
            " [49.27998074]\n",
            " [30.86055425]\n",
            " [44.97142806]\n",
            " [41.77576995]\n",
            " [28.29326042]\n",
            " [31.91961211]\n",
            " [47.1795411 ]\n",
            " [46.71268691]\n",
            " [59.62066886]\n",
            " [50.91356619]\n",
            " [44.75608127]\n",
            " [61.50576171]\n",
            " [58.4001009 ]\n",
            " [35.58212442]\n",
            " [37.21570987]\n",
            " [32.53030031]\n",
            " [45.92281253]\n",
            " [65.70664099]\n",
            " [66.96336956]\n",
            " [30.23218997]\n",
            " [40.0883478 ]\n",
            " [50.8958901 ]\n",
            " [52.06262137]\n",
            " [53.84004082]\n",
            " [51.52425439]\n",
            " [40.82438548]\n",
            " [54.79142529]\n",
            " [55.94048046]\n",
            " [50.15985243]\n",
            " [46.91035761]\n",
            " [57.68173931]\n",
            " [45.33060886]\n",
            " [59.45915876]\n",
            " [59.13613858]\n",
            " [45.81513914]\n",
            " [54.73758859]\n",
            " [32.06344612]\n",
            " [35.33061702]\n",
            " [51.0035635 ]\n",
            " [53.78620412]\n",
            " [50.48287261]\n",
            " [40.87822218]\n",
            " [49.33381744]\n",
            " [51.57809109]\n",
            " [38.77778254]\n",
            " [68.22009812]\n",
            " [47.86174208]\n",
            " [54.05538761]\n",
            " [32.42262691]\n",
            " [41.88344334]\n",
            " [49.60300093]\n",
            " [45.54595565]\n",
            " [63.12167107]\n",
            " [67.21487696]\n",
            " [53.89387752]\n",
            " [46.28199333]\n",
            " [53.01400583]\n",
            " [64.91676661]\n",
            " [37.62872736]\n",
            " [37.05419978]\n",
            " [42.31413693]\n",
            " [53.78620412]\n",
            " [43.19400861]\n",
            " [44.61224727]\n",
            " [55.68897306]\n",
            " [49.74683493]\n",
            " [23.51785355]\n",
            " [26.71351166]\n",
            " [51.16507359]\n",
            " [37.52105397]\n",
            " [50.23136521]\n",
            " [45.81513914]\n",
            " [41.88344334]\n",
            " [62.81632697]\n",
            " [67.59173384]\n",
            " [43.19400861]\n",
            " [42.51180763]\n",
            " [29.1731321 ]\n",
            " [64.70141982]\n",
            " [50.32136252]\n",
            " [41.61425986]\n",
            " [45.54595565]\n",
            " [47.91557878]\n",
            " [52.79865904]\n",
            " [45.13293816]\n",
            " [32.6918104 ]\n",
            " [38.83161923]\n",
            " [39.9445138 ]\n",
            " [48.54394306]\n",
            " [61.5595984 ]\n",
            " [49.69299824]\n",
            " [30.75288085]\n",
            " [25.97747398]\n",
            " [54.30689501]\n",
            " [32.90715719]\n",
            " [56.73035484]\n",
            " [39.26231281]\n",
            " [58.2024302 ]\n",
            " [31.59659193]\n",
            " [44.12771699]\n",
            " [41.68577264]\n",
            " [40.71671208]\n",
            " [54.4684051 ]\n",
            " [35.22294362]\n",
            " [34.27155916]\n",
            " [47.2333778 ]\n",
            " [47.75406869]\n",
            " [56.10199055]\n",
            " [32.6379737 ]\n",
            " [57.14337233]\n",
            " [48.16708618]\n",
            " [36.15665201]\n",
            " [24.93609221]\n",
            " [56.56884475]\n",
            " [32.17111951]\n",
            " [51.38042038]\n",
            " [28.65244121]\n",
            " [39.7830037 ]\n",
            " [41.61425986]\n",
            " [49.22614404]\n",
            " [38.04174486]\n",
            " [58.25626689]\n",
            " [49.80067163]\n",
            " [39.42382291]\n",
            " [32.90715719]\n",
            " [68.22009812]\n",
            " [44.39690048]\n",
            " [55.04293269]\n",
            " [58.4001009 ]\n",
            " [31.18357443]\n",
            " [50.8597295 ]\n",
            " [43.6608628 ]\n",
            " [26.40816757]\n",
            " [48.70545316]\n",
            " [53.21167654]\n",
            " [38.68778523]\n",
            " [58.41777699]\n",
            " [45.65362905]\n",
            " [42.04495344]\n",
            " [36.94652638]\n",
            " [41.36275246]\n",
            " [53.35551054]\n",
            " [47.64639529]\n",
            " [45.18677485]\n",
            " [68.22009812]\n",
            " [58.56161099]\n",
            " [42.97866182]\n",
            " [46.17431993]\n",
            " [44.61224727]\n",
            " [35.25910423]\n",
            " [35.63596112]\n",
            " [21.4712506 ]\n",
            " [53.21167654]\n",
            " [29.96300648]\n",
            " [38.31092834]\n",
            " [49.22614404]\n",
            " [55.36595288]\n",
            " [47.86174208]\n",
            " [52.42180216]\n",
            " [55.52746297]\n",
            " [37.21570987]\n",
            " [58.25626689]\n",
            " [51.32658369]\n",
            " [54.79142529]\n",
            " [42.29646084]\n",
            " [28.43709442]\n",
            " [42.56564433]\n",
            " [32.65564979]\n",
            " [35.52828772]\n",
            " [40.89589827]\n",
            " [41.56042316]\n",
            " [45.38444556]\n",
            " [51.0574002 ]\n",
            " [45.04294085]\n",
            " [49.22614404]\n",
            " [45.29444825]\n",
            " [34.43306925]\n",
            " [58.50777429]\n",
            " [48.90312386]\n",
            " [60.55437724]\n",
            " [27.50338604]\n",
            " [49.33381744]\n",
            " [49.90834503]\n",
            " [41.14740567]\n",
            " [65.07827671]\n",
            " [49.38765414]\n",
            " [38.09558155]\n",
            " [47.64639529]\n",
            " [49.96218172]\n",
            " [42.18878744]\n",
            " [37.79023746]\n",
            " [50.06985512]\n",
            " [45.24061155]\n",
            " [58.99230457]\n",
            " [53.42702333]\n",
            " [68.11242473]\n",
            " [39.04696602]\n",
            " [53.62469403]\n",
            " [41.72193325]\n",
            " [54.5760785 ]\n",
            " [57.98708341]\n",
            " [33.53552148]\n",
            " [37.43105666]\n",
            " [52.20645537]\n",
            " [55.99431716]\n",
            " [59.67450555]\n",
            " [41.45274976]\n",
            " [53.42702333]\n",
            " [51.57809109]\n",
            " [42.51180763]\n",
            " [40.46520468]\n",
            " [43.03249852]\n",
            " [29.54998898]\n",
            " [68.11242473]\n",
            " [47.80790538]\n",
            " [39.8906771 ]\n",
            " [38.93929263]\n",
            " [47.1257044 ]\n",
            " [52.94249305]\n",
            " [50.6443827 ]\n",
            " [46.33583003]\n",
            " [36.3181621 ]\n",
            " [57.82557331]\n",
            " [45.54595565]\n",
            " [50.37519922]\n",
            " [43.08633521]\n",
            " [50.42903591]\n",
            " [31.12973774]\n",
            " [44.75608127]\n",
            " [43.39167931]\n",
            " [42.76331503]\n",
            " [62.45714618]\n",
            " [40.82438548]\n",
            " [50.8058928 ]\n",
            " [39.24463673]\n",
            " [47.1795411 ]\n",
            " [56.42501074]\n",
            " [49.63916154]\n",
            " [27.91640353]\n",
            " [51.79343788]\n",
            " [55.41978957]\n",
            " [48.70545316]\n",
            " [68.22009812]\n",
            " [44.86375467]\n",
            " [45.18677485]\n",
            " [27.61105944]\n",
            " [39.62149361]\n",
            " [28.18558702]\n",
            " [53.37318663]\n",
            " [48.07708887]\n",
            " [47.64639529]\n",
            " [51.32658369]\n",
            " [62.24179939]\n",
            " [42.40413423]\n",
            " [29.81917247]\n",
            " [64.91676661]\n",
            " [44.45073718]\n",
            " [47.34105119]\n",
            " [55.67129697]\n",
            " [50.59054601]\n",
            " [37.21570987]\n",
            " [46.03048593]\n",
            " [68.22009812]\n",
            " [52.11645806]\n",
            " [43.08633521]\n",
            " [49.17230735]\n",
            " [50.53670931]\n",
            " [40.24985789]\n",
            " [52.36796546]\n",
            " [49.22614404]\n",
            " [38.43627783]\n",
            " [59.29764867]\n",
            " [63.80387205]\n",
            " [38.14941825]\n",
            " [45.50979504]\n",
            " [39.20847612]\n",
            " [44.43306109]\n",
            " [47.80790538]\n",
            " [48.59777976]\n",
            " [55.99431716]\n",
            " [40.82438548]\n",
            " [66.11965848]\n",
            " [57.03569894]\n",
            " [48.07708887]\n",
            " [34.16388576]\n",
            " [35.42061433]\n",
            " [67.05336686]\n",
            " [63.08551046]\n",
            " [54.30689501]\n",
            " [50.8058928 ]\n",
            " [48.22092288]\n",
            " [54.6299152 ]\n",
            " [50.91356619]\n",
            " [43.08633521]\n",
            " [49.01079725]\n",
            " [28.70627791]\n",
            " [44.98910415]\n",
            " [47.75406869]\n",
            " [56.37117404]\n",
            " [58.72312108]\n",
            " [39.56765691]\n",
            " [57.93324671]\n",
            " [58.0947568 ]\n",
            " [53.78620412]\n",
            " [32.6379737 ]\n",
            " [47.2333778 ]\n",
            " [53.37318663]\n",
            " [62.60098018]\n",
            " [40.35753129]\n",
            " [35.68979782]\n",
            " [43.14017191]\n",
            " [52.38564155]\n",
            " [35.58212442]\n",
            " [33.53552148]\n",
            " [42.92482512]\n",
            " [61.03890752]\n",
            " [34.89992344]\n",
            " [59.87217626]\n",
            " [61.18274152]\n",
            " [61.12890482]\n",
            " [41.77576995]\n",
            " [41.72193325]\n",
            " [50.9497268 ]\n",
            " [40.78822487]\n",
            " [47.95173939]\n",
            " [53.51702063]\n",
            " [34.28923524]\n",
            " [31.21973504]\n",
            " [41.14740567]\n",
            " [48.02325218]\n",
            " [62.81632697]\n",
            " [53.26551323]\n",
            " [58.56161099]\n",
            " [57.30488242]\n",
            " [49.60300093]\n",
            " [47.91557878]\n",
            " [51.63192778]\n",
            " [57.61022652]\n",
            " [54.68375189]\n",
            " [31.54275523]\n",
            " [31.11206165]\n",
            " [38.68778523]\n",
            " [52.00878467]\n",
            " [24.20005453]\n",
            " [39.54998082]\n",
            " [33.37401138]\n",
            " [46.65885021]\n",
            " [38.83161923]\n",
            " [46.55117682]\n",
            " [55.09676939]\n",
            " [53.78620412]\n",
            " [46.08432263]\n",
            " [26.28281808]\n",
            " [52.58331225]\n",
            " [54.30689501]\n",
            " [48.59777976]\n",
            " [35.74363451]\n",
            " [29.38847889]\n",
            " [57.41255582]\n",
            " [58.50777429]\n",
            " [61.20041761]\n",
            " [39.99835049]\n",
            " [45.86897584]\n",
            " [55.77897037]\n",
            " [56.73035484]\n",
            " [32.65564979]\n",
            " [42.24262414]\n",
            " [29.60382568]\n",
            " [36.3719988 ]\n",
            " [47.2872145 ]\n",
            " [63.28318116]\n",
            " [30.50137345]\n",
            " [54.84526199]\n",
            " [35.9589813 ]\n",
            " [52.74482235]\n",
            " [54.84526199]\n",
            " [56.67651814]\n",
            " [47.01803101]\n",
            " [45.81513914]\n",
            " [50.53670931]\n",
            " [51.57809109]\n",
            " [47.07186771]\n",
            " [48.22092288]\n",
            " [34.89992344]\n",
            " [45.02526476]\n",
            " [56.15582725]\n",
            " [46.22815663]\n",
            " [57.25104573]\n",
            " [37.73640076]\n",
            " [37.26954657]\n",
            " [61.34425161]\n",
            " [44.97142806]\n",
            " [35.47445102]\n",
            " [54.5760785 ]\n",
            " [49.85450833]\n",
            " [44.03771968]\n",
            " [ 9.81999723]\n",
            " [49.54916423]\n",
            " [44.34306378]\n",
            " [50.59054601]\n",
            " [47.75406869]\n",
            " [54.30689501]\n",
            " [54.3784078 ]\n",
            " [48.95696056]\n",
            " [52.63714895]\n",
            " [56.82035215]\n",
            " [53.53469672]\n",
            " [46.33583003]\n",
            " [49.69299824]\n",
            " [50.8058928 ]\n",
            " [32.90715719]\n",
            " [36.42583549]\n",
            " [31.59659193]\n",
            " [37.73640076]\n",
            " [46.76652361]\n",
            " [63.60620135]\n",
            " [53.73236742]\n",
            " [51.16507359]\n",
            " [54.36073171]]\n",
            "\n",
            "Predicted Y values (Testing):\n",
            "[[47.07186771]\n",
            " [27.75489344]\n",
            " [39.15463942]\n",
            " [38.36476504]\n",
            " [60.24903314]\n",
            " [47.75406869]\n",
            " [68.16626143]\n",
            " [27.82640623]\n",
            " [39.67533031]\n",
            " [55.77897037]\n",
            " [55.58129967]\n",
            " [42.56564433]\n",
            " [43.301682  ]\n",
            " [36.42583549]\n",
            " [40.24985789]\n",
            " [34.43306925]\n",
            " [44.66608397]\n",
            " [42.67331772]\n",
            " [60.71588733]\n",
            " [42.45797093]\n",
            " [53.42702333]\n",
            " [47.96941548]\n",
            " [42.51180763]\n",
            " [50.48287261]\n",
            " [38.88545593]\n",
            " [57.52022921]\n",
            " [34.89992344]\n",
            " [39.36998621]\n",
            " [56.10199055]\n",
            " [33.78702888]\n",
            " [65.5451309 ]\n",
            " [52.27796816]\n",
            " [21.8304314 ]\n",
            " [38.14941825]\n",
            " [56.83802823]\n",
            " [53.10400314]\n",
            " [65.11443732]\n",
            " [61.75726911]\n",
            " [46.76652361]\n",
            " [53.37318663]\n",
            " [58.75928169]\n",
            " [41.03973227]\n",
            " [45.38444556]\n",
            " [50.37519922]\n",
            " [38.99312933]\n",
            " [58.3462642 ]\n",
            " [52.90633244]\n",
            " [52.96016914]\n",
            " [26.60583827]\n",
            " [53.94771422]\n",
            " [33.06866729]\n",
            " [49.17230735]\n",
            " [42.40413423]\n",
            " [50.21368912]\n",
            " [33.58935817]\n",
            " [50.6443827 ]\n",
            " [26.44432818]\n",
            " [43.93004629]\n",
            " [51.1112369 ]\n",
            " [57.57406591]\n",
            " [31.59659193]\n",
            " [34.11004906]\n",
            " [45.38444556]\n",
            " [39.99835049]\n",
            " [36.0666547 ]\n",
            " [61.9726159 ]\n",
            " [68.22009812]\n",
            " [44.61224727]\n",
            " [40.98589557]\n",
            " [59.29764867]\n",
            " [33.15866459]\n",
            " [68.16626143]\n",
            " [42.29646084]\n",
            " [59.65682947]\n",
            " [42.51180763]\n",
            " [36.74885568]\n",
            " [53.42702333]\n",
            " [31.27357174]\n",
            " [48.27475958]\n",
            " [36.98268699]\n",
            " [49.96218172]\n",
            " [49.53148814]\n",
            " [43.44551601]\n",
            " [56.62268144]\n",
            " [50.21368912]\n",
            " [55.99431716]\n",
            " [53.42702333]\n",
            " [42.87098842]\n",
            " [43.08633521]\n",
            " [50.42903591]\n",
            " [48.43626967]\n",
            " [45.65362905]\n",
            " [40.1960212 ]\n",
            " [39.15463942]\n",
            " [14.91842428]\n",
            " [61.23657822]\n",
            " [54.5222418 ]\n",
            " [55.15060609]\n",
            " [53.48086003]\n",
            " [45.97664923]\n",
            " [50.6982194 ]\n",
            " [22.40495899]\n",
            " [59.02846518]\n",
            " [19.17314026]\n",
            " [64.91676661]\n",
            " [55.72513367]\n",
            " [45.81513914]\n",
            " [47.55639798]\n",
            " [49.63916154]\n",
            " [39.15463942]\n",
            " [50.8958901 ]\n",
            " [63.08551046]\n",
            " [50.53670931]\n",
            " [50.8058928 ]\n",
            " [60.60821394]\n",
            " [55.40211348]\n",
            " [45.86897584]\n",
            " [42.56564433]\n",
            " [68.22009812]\n",
            " [34.95376014]\n",
            " [43.4093554 ]\n",
            " [68.11242473]\n",
            " [55.41978957]\n",
            " [41.82960665]\n",
            " [28.70627791]\n",
            " [31.95577272]\n",
            " [35.74363451]\n",
            " [57.08953563]\n",
            " [49.36997805]\n",
            " [42.35029753]\n",
            " [45.59979235]\n",
            " [60.60821394]\n",
            " [52.27796816]\n",
            " [47.96941548]\n",
            " [37.46721727]\n",
            " [30.28602666]\n",
            " [49.63916154]\n",
            " [67.32255035]\n",
            " [56.99953833]\n",
            " [32.7456471 ]\n",
            " [48.90312386]\n",
            " [21.25590381]\n",
            " [55.52746297]\n",
            " [36.3181621 ]\n",
            " [50.21368912]\n",
            " [41.82960665]\n",
            " [40.89589827]\n",
            " [43.08633521]\n",
            " [35.90514461]\n",
            " [46.80268422]\n",
            " [43.14017191]\n",
            " [40.57287808]\n",
            " [57.77173661]\n",
            " [63.80387205]\n",
            " [52.20645537]\n",
            " [29.60382568]\n",
            " [52.96016914]\n",
            " [40.24985789]\n",
            " [39.40614682]\n",
            " [63.06783437]\n",
            " [50.62670662]\n",
            " [49.33381744]\n",
            " [55.88664376]\n",
            " [63.96538214]\n",
            " [32.6379737 ]\n",
            " [38.25709165]\n",
            " [52.58331225]\n",
            " [52.74482235]\n",
            " [51.27274699]\n",
            " [38.25709165]\n",
            " [55.94048046]\n",
            " [43.23016922]\n",
            " [48.38243297]\n",
            " [59.62066886]\n",
            " [27.09036855]\n",
            " [30.48369737]\n",
            " [58.72312108]\n",
            " [44.86375467]\n",
            " [47.1257044 ]\n",
            " [57.28720634]\n",
            " [54.79142529]\n",
            " [34.05621236]\n",
            " [52.88865635]\n",
            " [49.38765414]\n",
            " [28.23942372]\n",
            " [53.94771422]\n",
            " [23.19483336]\n",
            " [42.24262414]\n",
            " [51.0035635 ]\n",
            " [48.59777976]\n",
            " [39.40614682]\n",
            " [42.13495074]\n",
            " [42.29646084]\n",
            " [51.79343788]\n",
            " [42.60180493]\n",
            " [45.76130244]\n",
            " [49.99834233]\n",
            " [56.76651545]\n",
            " [54.30689501]\n",
            " [50.59054601]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent from Scratch"
      ],
      "metadata": {
        "id": "pq5qDp_10rBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'Math': [48, 62, 79, 76, 59],\n",
        "    'Reading': [68, 81, 80, 83, 64],\n",
        "    'Writing': [63, 72, 78, 79, 62]\n",
        "}\n",
        "\n",
        "# Convert the dataset to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare the feature matrix (X) and target vector (Y)\n",
        "X = df[['Math', 'Reading']].values  # Features: Math and Reading scores\n",
        "Y = df[['Writing']].values          # Target: Writing scores\n",
        "\n",
        "# Add a column of ones for the bias term in X\n",
        "X = np.c_[np.ones(X.shape[0]), X]    # Adding bias (intercept term)\n",
        "\n",
        "# Initialize weights (W)\n",
        "W = np.zeros((X.shape[1], 1))  # Initializing weights to zero\n",
        "\n",
        "# Gradient Descent Parameters\n",
        "alpha = 0.01  # Learning rate\n",
        "iterations = 1000  # Number of iterations\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "    W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "    cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)  # Linear hypothesis: X * W\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y  # Residuals: predicted Y minus actual Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)  # Gradient of cost function\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W - alpha * dw  # Update rule: W = W - alpha * dw\n",
        "\n",
        "        # Step 5: New Cost Value (Mean Squared Error)\n",
        "        cost = (1 / (2 * m)) * np.sum(loss ** 2)  # Cost function (MSE)\n",
        "        cost_history[iteration] = cost  # Store cost value\n",
        "\n",
        "        # Update W for the next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Run gradient descent\n",
        "W_final, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Display the results\n",
        "print(\"Final Weights (W):\")\n",
        "print(W_final)\n",
        "\n",
        "print(\"\\nCost History (last 10 values):\")\n",
        "print(cost_history[-10:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9IZ2Sk80yyU",
        "outputId": "efba05e9-e8d2-4b06-a981-b4e4704da398"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights (W):\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "\n",
            "Cost History (last 10 values):\n",
            "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-995e681f8677>:64: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum(loss ** 2)  # Cost function (MSE)\n",
            "<ipython-input-14-995e681f8677>:61: RuntimeWarning: invalid value encountered in subtract\n",
            "  W_update = W - alpha * dw  # Update rule: W = W - alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random test data\n",
        "np.random.seed(0) # For reproducibility\n",
        "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGRDQAOw03fz",
        "outputId": "f9b1594f-5889-462d-ab5b-e5ce0853af7c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History: [0.10788797459582472, 0.10711197094660153, 0.10634880599939901, 0.10559826315680618, 0.10486012948320558, 0.1041341956428534, 0.10342025583900626, 0.1027181077540776, 0.1020275524908062, 0.10134839451441931, 0.1006804415957737, 0.1000235047554587, 0.09937739820884377, 0.09874193931205609, 0.09811694850887098, 0.09750224927850094, 0.0968976680842672, 0.09630303432313951, 0.09571818027612913, 0.09514294105952065, 0.09457715457692842, 0.09402066147216397, 0.09347330508290017, 0.09293493139511913, 0.09240538899833017, 0.09188452904154543, 0.0913722051899995, 0.09086827358260123, 0.09037259279010502, 0.08988502377398919, 0.08940542984603007, 0.08893367662855953, 0.08846963201539432, 0.08801316613342668, 0.08756415130486386, 0.08712246201010665, 0.08668797485125508, 0.08626056851623207, 0.08584012374351278, 0.08542652328745133, 0.08501965188419301, 0.0846193962181636, 0.08422564488912489, 0.08383828837978763, 0.08345721902397185, 0.08308233097530582, 0.08271352017645425, 0.08235068432886682, 0.08199372286303817, 0.08164253690927113, 0.08129702926893387, 0.08095710438620353, 0.08062266832028739, 0.08029362871811391, 0.07996989478748553, 0.0796513772706855, 0.07933798841853089, 0.07902964196486459, 0.07872625310147845, 0.07842773845346054, 0.07813401605495938, 0.0778450053253578, 0.0775606270458499, 0.07728080333641404, 0.07700545763317514, 0.07673451466614989, 0.07646790043736812, 0.07620554219936448, 0.07594736843403344, 0.07569330883184205, 0.07544329427139428, 0.07519725679934074, 0.07495512961062821, 0.07471684702908327, 0.07448234448832412, 0.0742515585129952, 0.07402442670031911, 0.0738008877019607, 0.07358088120619749, 0.0733643479203919, 0.07315122955375959, 0.07294146880042966, 0.07273500932279067, 0.07253179573511871, 0.07233177358748233, 0.0721348893499193, 0.07194109039688139, 0.07175032499194182, 0.07156254227276149, 0.07137769223630935, 0.07119572572433286, 0.07101659440907385, 0.07084025077922623, 0.070666648126131, 0.07049574053020462, 0.07032748284759716, 0.07016183069707572, 0.0699987404471299, 0.06983816920329523, 0.06968007479569092, 0.06952441576676843, 0.06937115135926715, 0.06922024150437375, 0.06907164681008185, 0.06892532854974835, 0.0687812486508435, 0.06863936968389095, 0.06849965485159508, 0.06836206797815195, 0.06822657349874123, 0.06809313644919561, 0.067961722455845, 0.06783229772553254, 0.06770482903579932, 0.06757928372523506, 0.06745562968399212, 0.06733383534445969, 0.06721386967209597, 0.06709570215641501, 0.06697930280212627, 0.06686464212042395, 0.06675169112042348, 0.0666404213007429, 0.06653080464122665, 0.06642281359480932, 0.06631642107951677, 0.06621160047060279, 0.06610832559281864, 0.06600657071281309, 0.0659063105316614, 0.06580752017752023, 0.06571017519840698, 0.06561425155510119, 0.06551972561416586, 0.06542657414108709, 0.06533477429352925, 0.06524430361470467, 0.06515514002685512, 0.06506726182484374, 0.06498064766985515, 0.06489527658320228, 0.06481112794023773, 0.06472818146436811, 0.0646464172211699, 0.06456581561260431, 0.06448635737133043, 0.0644080235551142, 0.06433079554133217, 0.06425465502156798, 0.06417958399630046, 0.06410556476968135, 0.06403257994440141, 0.0639606124166433, 0.06388964537111992, 0.06381966227619645, 0.06375064687909507, 0.06368258320118077, 0.06361545553332655, 0.06354924843135755, 0.06348394671157162, 0.06341953544633615, 0.06335599995975896, 0.06329332582343267, 0.06323149885225086, 0.06317050510029515, 0.06311033085679153, 0.06305096264213547, 0.06299238720398384, 0.0629345915134133, 0.06287756276114324, 0.06282128835382297, 0.0627657559103815, 0.06271095325843898, 0.06265686843077901, 0.06260348966188053, 0.06255080538450809, 0.06249880422636036, 0.06244747500677472, 0.06239680673348793, 0.06234678859945137, 0.06229740997970036, 0.0622486604282762, 0.06220052967520031, 0.062153007623499706, 0.062106084346282515, 0.062059750083863094, 0.06201399524093575, 0.06196881038379625, 0.061924186237610215, 0.06188011368372787, 0.0618365837570441, 0.06179358764340313, 0.061751116677047156, 0.06170916233810801, 0.0616677162501414, 0.06162677017770278, 0.061586316023964055, 0.0615463458283708, 0.06150685176433905, 0.06146782613699094, 0.0614292613809287, 0.061391150058046254, 0.06135348485537795, 0.06131625858298352, 0.061279464171868706, 0.06124309467194143, 0.061207143250002184, 0.06117160318776841, 0.06113646787993252, 0.061101730832252524, 0.06106738565967507, 0.06103342608449018, 0.06099984593451716, 0.06096663914132128, 0.0609337997384604, 0.0609013218597616, 0.06086919973762659, 0.06083742770136588, 0.06080600017556133, 0.06077491167845612, 0.06074415682037193, 0.06071373030215326, 0.060683626913637524, 0.06065384153215141, 0.06062436912103256, 0.0605952047281761, 0.06056634348460599, 0.060537780603070336, 0.060509511376660545, 0.0604815311774538, 0.060453835455178496, 0.06042641973590228, 0.06039927962074216, 0.060372410784596583, 0.060345808974898815, 0.06031947001039151, 0.06029338977992186, 0.06026756424125725, 0.060241989419920934, 0.06021666140804729, 0.0601915763632565, 0.06016673050754826, 0.060142120126214255, 0.06011774156676883, 0.06009359123789796, 0.06006966560842588, 0.06004596120629915, 0.060022474617588105, 0.059999202485504784, 0.059976141509438, 0.05995328844400421, 0.05993064009811483, 0.05990819333405906, 0.059885945066602345, 0.059863892262100066, 0.059842031937626106, 0.059820361160116395, 0.05979887704552664, 0.05977757675800453, 0.05975645750907579, 0.05973551655684408, 0.0597147512052044, 0.05969415880306974, 0.05967373674361096, 0.05965348246350928, 0.05963339344222168, 0.059613467201258485, 0.059593701303473294, 0.05957409335236496, 0.05955464099139111, 0.05953534190329372, 0.059516193809435625, 0.0594971944691485, 0.0594783416790919, 0.05945963327262296, 0.0594410671191769, 0.05942264112365792, 0.05940435322584049, 0.059386201399780576, 0.059368183653237094, 0.059350298027102844, 0.05933254259484533, 0.05931491546195686, 0.05929741476541398, 0.0592800386731462, 0.05926278538351338, 0.05924565312479226, 0.05922864015467154, 0.059211744759755505, 0.059194965255076046, 0.05917829998361292, 0.05916174731582212, 0.059145305649172315, 0.059128973407688926, 0.059112749041506096, 0.05909663102642617, 0.05908061786348662, 0.059064708078534194, 0.05904890022180654, 0.05903319286752055, 0.05901758461346795, 0.05900207408061755, 0.058986659912724324, 0.05897134077594505, 0.058956115358460404, 0.05894098237010357, 0.05892594054199501, 0.05891098862618344, 0.05889612539529293, 0.05888134964217589, 0.05886666017957195, 0.058852055839772675, 0.05883753547429179, 0.058823097953541174, 0.058808742166512155, 0.05879446702046235, 0.058780271440607684, 0.05876615436981961, 0.05875211476832761, 0.05873815161342641, 0.05872426389918856, 0.058710450636181515, 0.05869671085118971, 0.058683043586941104, 0.058669447901838714, 0.05865592286969638, 0.05864246757947903, 0.05862908113504752, 0.05861576265490756, 0.058602511271963004, 0.05858932613327336, 0.058576206399815284, 0.05856315124624814, 0.05855015986068357, 0.05853723144445888, 0.05852436521191438, 0.05851156039017436, 0.0584988162189318, 0.05848613195023677, 0.05847350684828838, 0.058460940189230176, 0.05844843126094919, 0.05843597936287807, 0.05842358380580092, 0.05841124391166213, 0.05839895901337858, 0.05838672845465502, 0.05837455158980245, 0.05836242778355972, 0.05835035641091811, 0.05833833685694878, 0.05832636851663321, 0.05831445079469655, 0.05830258310544367, 0.05829076487259809, 0.05827899552914358, 0.05826727451716845, 0.058255601287712386, 0.0582439753006161, 0.058232396024373266, 0.05822086293598511, 0.058209375520817355, 0.058197933272459756, 0.05818653569258779, 0.05817518229082684, 0.058163872584618616, 0.05815260609908985, 0.058141382366923164, 0.058130200928230166, 0.058119061330426776, 0.058107963128110396, 0.05809690588293942, 0.058085889163514745, 0.058074912545263056, 0.058063975610322414, 0.058053077947429504, 0.05804221915180897, 0.05803139882506447, 0.05802061657507169, 0.0580098720158732, 0.05799916476757483, 0.057988494456244134, 0.057977860713810364, 0.057967263177966154, 0.05795670149207094, 0.05794617530505593, 0.05793568427133074, 0.057925228050691516, 0.057914806308230836, 0.057904418714248757, 0.057894064944165734, 0.05788374467843681, 0.05787345760246728, 0.057863203406529826, 0.05785298178568306, 0.05784279243969133, 0.057832635072946004, 0.05782250939438805, 0.0578124151174319, 0.05780235195989063, 0.057792319643902336, 0.05778231789585793, 0.0577723464463298, 0.05776240503000217, 0.05775249338560223, 0.05774261125583255, 0.05773275838730474, 0.05772293453047407, 0.05771313943957533, 0.0577033728725597, 0.057693634591032654, 0.057683924360193005, 0.05767424194877295, 0.05766458712897906, 0.05765495967643434, 0.057645359370121226, 0.05763578599232564, 0.057626239328581796, 0.05761671916761811, 0.05760722530130401, 0.05759775752459752, 0.057588315635493874, 0.057578899434974906, 0.05756950872695933, 0.05756014331825381, 0.05755080301850501, 0.057541487640152184, 0.05753219699838088, 0.057522930911077144, 0.057513689198782685, 0.05750447168465076, 0.05749527819440267, 0.057486108556285255, 0.05747696260102877, 0.05746784016180581, 0.05745874107419066, 0.05744966517611951, 0.05744061230785123, 0.05743158231192885, 0.05742257503314173, 0.057413590318488285, 0.05740462801713937, 0.057395687980402336, 0.05738677006168561, 0.05737787411646401, 0.057369000002244354, 0.05736014757853204, 0.05735131670679789, 0.057342507250445686, 0.05733371907478018, 0.05732495204697581, 0.057316206036045626, 0.05730748091281111, 0.057298776549872255, 0.05729009282157824, 0.05728142960399854, 0.05727278677489465, 0.05726416421369212, 0.05725556180145319, 0.05724697942084986, 0.0572384169561373, 0.05722987429312795, 0.05722135131916572, 0.05721284792310103, 0.05720436399526589, 0.0571958994274496, 0.057187454112874896, 0.05717902794617434, 0.05717062082336728, 0.057162232641836994, 0.05715386330030848, 0.05714551269882637, 0.05713718073873334, 0.05712886732264895, 0.05712057235444866, 0.05711229573924336, 0.05710403738335914, 0.0570957971943175, 0.05708757508081579, 0.057079370952708035, 0.057071184720986066, 0.05706301629776107, 0.057054865596245175, 0.057046732530733744, 0.05703861701658757, 0.05703051897021566, 0.05702243830905818, 0.057014374951569684, 0.057006328817202634, 0.05699829982639134, 0.05699028790053585, 0.05698229296198646, 0.05697431493402824, 0.05696635374086599, 0.05695840930760929, 0.056950481560257914, 0.0569425704256875, 0.0569346758316353, 0.05692679770668645, 0.05691893598026014, 0.05691109058259633, 0.05690326144474244, 0.05689544849854041, 0.056887651676613984, 0.05687987091235604, 0.056872106139916355, 0.05686435729418944, 0.05685662431080263, 0.0568489071261043, 0.05684120567715239, 0.056833519901703065, 0.05682584973819958, 0.05681819512576124, 0.05681055600417275, 0.056802932313873525, 0.056795323995947306, 0.056787730992111936, 0.05678015324470926, 0.05677259069669528, 0.05676504329163035, 0.05675751097366966, 0.05674999368755382, 0.05674249137859953, 0.05673500399269066, 0.05672753147626912, 0.056720073776326194, 0.05671263084039382, 0.056705202616536096, 0.05669778905334098, 0.05669039009991206, 0.05668300570586036, 0.05667563582129657, 0.056668280396823104, 0.05666093938352648, 0.05665361273296975, 0.056646300397185066, 0.05663900232866641, 0.05663171848036241, 0.05662444880566923, 0.05661719325842369, 0.056609951792896414, 0.05660272436378514, 0.05659551092620811, 0.056588311435697584, 0.05658112584819342, 0.05657395412003692, 0.05656679620796451, 0.05655965206910184, 0.05655252166095763, 0.05654540494141801, 0.056538301868740586, 0.05653121240154893, 0.056524136498826864, 0.056517074119913024, 0.05651002522449555, 0.05650298977260663, 0.05649596772461748, 0.056488959041233064, 0.05648196368348717, 0.05647498161273735, 0.0564680127906601, 0.056461057179246134, 0.05645411474079556, 0.05644718543791332, 0.05644026923350467, 0.056433366090770515, 0.05642647597320331, 0.05641959884458242, 0.05641273466897008, 0.05640588341070717, 0.05639904503440896, 0.05639221950496121, 0.05638540678751615, 0.05637860684748858, 0.056371819650551894, 0.05636504516263454, 0.05635828334991603, 0.05635153417882347, 0.05634479761602789, 0.05633807362844066, 0.05633136218321008, 0.056324663247717996, 0.05631797678957624, 0.05631130277662355, 0.05630464117692215, 0.056297991958754665, 0.05629135509062089, 0.056284730541234666, 0.05627811827952098, 0.05627151827461283, 0.0562649304958483, 0.05625835491276777, 0.05625179149511093, 0.05624524021281401, 0.05623870103600713, 0.05623217393501149, 0.05622565888033667, 0.05621915584267811, 0.05621266479291449, 0.056206185702105234, 0.05619971854148787, 0.05619326328247578, 0.05618681989665565, 0.05618038835578517, 0.05617396863179063, 0.05616756069676472, 0.05616116452296419, 0.05615478008280768, 0.05614840734887347, 0.05614204629389748, 0.05613569689077096, 0.0561293591125386, 0.056123032932396344, 0.05611671832368947, 0.05611041525991056, 0.05610412371469758, 0.056097843661832, 0.05609157507523687, 0.05608531792897493, 0.05607907219724691, 0.056072837854389615, 0.05606661487487427, 0.05606040323330473, 0.056054202904415734, 0.05604801386307135, 0.056041836084263226, 0.056035669543109, 0.056029514214850695, 0.05602337007485319, 0.05601723709860266, 0.05601111526170498, 0.05600500453988435, 0.05599890490898177, 0.05599281634495359, 0.055986738823870105, 0.055980672321914095, 0.055974616815379595, 0.055968572280670384, 0.055962538694298715, 0.05595651603288404, 0.05595050427315166, 0.0559445033919315, 0.05593851336615685, 0.05593253417286313, 0.05592656578918666, 0.05592060819236354, 0.05591466135972839, 0.05590872526871329, 0.05590279989684662, 0.05589688522175184, 0.055890981221146614, 0.05588508787284151, 0.055879205154739084, 0.05587333304483278, 0.05586747152120588, 0.055861620562030555, 0.05585578014556684, 0.05584995025016163, 0.05584413085424776, 0.05583832193634303, 0.0558325234750493, 0.0558267354490515, 0.05582095783711688, 0.05581519061809395, 0.05580943377091164, 0.055803687274578614, 0.05579795110818212, 0.05579222525088745, 0.055786509681936956, 0.05578080438064925, 0.05577510932641848, 0.05576942449871346, 0.055763749877076975, 0.05575808544112503, 0.05575243117054599, 0.05574678704509999, 0.05574115304461813, 0.055735529149001775, 0.05572991533822185, 0.0557243115923182, 0.05571871789139883, 0.05571313421563932, 0.05570756054528211, 0.055701996860635865, 0.055696443142074864, 0.055690899370038335, 0.05568536552502988, 0.05567984158761686, 0.055674327538429685, 0.05566882335816142, 0.055663329027567085, 0.055657844527463085, 0.05565236983872668, 0.05564690494229538, 0.0556414498191665, 0.05563600445039652, 0.055630568817100635, 0.05562514290045211, 0.05561972668168197, 0.05561432014207832, 0.05560892326298588, 0.055603536025805575, 0.055598158411993996, 0.05559279040306291, 0.055587431980578784, 0.055582083126162425, 0.05557674382148841, 0.055571414048284674, 0.05556609378833211, 0.055560783023464094, 0.05555548173556606, 0.055550189906575106, 0.05554490751847952, 0.055539634553318486, 0.05553437099318153, 0.055529116820208266, 0.05552387201658793, 0.055518636564558965, 0.05551341044640875, 0.05550819364447313, 0.05550298614113609, 0.05549778791882936, 0.05549259896003212, 0.05548741924727061, 0.05548224876311773, 0.055477087490192804, 0.0554719354111612, 0.055466792508733924, 0.05546165876566746, 0.055456534164763226, 0.05545141868886745, 0.05544631232087083, 0.05544121504370806, 0.055436126840357744, 0.055431047693841926, 0.05542597758722593, 0.055420916503617974, 0.05541586442616892, 0.055410821338071965, 0.05540578722256242, 0.05540076206291734, 0.055395745842455345, 0.05539073854453634, 0.05538574015256118, 0.05538075064997147, 0.055375770020249314, 0.05537079824691705, 0.05536583531353694, 0.05536088120371103, 0.05535593590108084, 0.05535099938932713, 0.055346071652169704, 0.0553411526733671, 0.05533624243671647, 0.05533134092605322, 0.055326448125250914, 0.05532156401822096, 0.05531668858891247, 0.055311821821311946, 0.055306963699443185, 0.05530211420736701, 0.055297273329180996, 0.05529244104901939, 0.0552876173510529, 0.05528280221948834, 0.05527799563856866, 0.055273197592572584, 0.05526840806581448, 0.055263627042644155, 0.055258854507446706, 0.05525409044464235, 0.05524933483868614, 0.05524458767406786, 0.05523984893531189, 0.055235118606976955, 0.05523039667365596, 0.05522568311997589, 0.055220977930597534, 0.055216281090215466, 0.05521159258355773, 0.055206912395385714, 0.055202240510494154, 0.05519757691371069, 0.055192921589895944, 0.055188274523943294, 0.05518363570077869, 0.05517900510536053, 0.05517438272267951, 0.055169768537758505, 0.055165162535652366, 0.055160564701447826, 0.05515597502026334, 0.055151393477248956, 0.05514682005758613, 0.055142254746487665, 0.05513769752919755, 0.0551331483909908, 0.055128607317173346, 0.055124074293081866, 0.055119549304083755, 0.05511503233557687, 0.05511052337298953, 0.05510602240178027, 0.05510152940743782, 0.05509704437548093, 0.05509256729145828, 0.05508809814094827, 0.05508363690955906, 0.05507918358292834, 0.05507473814672324, 0.055070300586640204, 0.05506587088840496, 0.05506144903777225, 0.05505703502052585, 0.055052628822478474, 0.05504823042947156, 0.05504383982737522, 0.05503945700208816, 0.05503508193953754, 0.055030714625678864, 0.05502635504649593, 0.05502200318800065, 0.055017659036233034, 0.055013322577261034, 0.055008993797180404, 0.05500467268211479, 0.05500035921821539, 0.054996053391661005, 0.05499175518865794, 0.05498746459543984, 0.054983181598267664, 0.0549789061834296, 0.054974638337240846, 0.05497037804604376, 0.0549661252962075, 0.054961880074128146, 0.0549576423662285, 0.054953412158958034, 0.054949189438792796, 0.05494497419223534, 0.05494076640581463, 0.05493656606608597, 0.05493237315963089, 0.05492818767305708, 0.05492400959299837, 0.05491983890611449, 0.05491567559909121, 0.05491151965864005, 0.05490737107149833, 0.05490322982442909, 0.054899095904220915, 0.05489496929768798, 0.05489084999166989, 0.05488673797303166, 0.054882633228663574, 0.05487853574548118, 0.054874445510425175, 0.05487036251046136, 0.054866286732580524, 0.05486221816379847, 0.05485815679115577, 0.0548541026017179, 0.054850055582575, 0.05484601572084191, 0.0548419830036581, 0.05483795741818747, 0.05483393895161846, 0.0548299275911639, 0.054825923324060916, 0.05482192613757092, 0.05481793601897949, 0.05481395295559637, 0.05480997693475535, 0.05480600794381422, 0.0548020459701547, 0.054798091001182415, 0.05479414302432678, 0.054790202027040935, 0.05478626799680179, 0.05478234092110976, 0.05477842078748891, 0.0547745075834868, 0.054770601296674395, 0.05476670191464608, 0.05476280942501958, 0.054758923815435796, 0.05475504507355896, 0.05475117318707636, 0.0547473081436984, 0.05474344993115854, 0.054739598537213205, 0.054735753949641676, 0.05473191615624621, 0.05472808514485178, 0.054724260903306156, 0.05472044341947975, 0.05471663268126573, 0.05471282867657969, 0.0547090313933599, 0.05470524081956701, 0.05470145694318413, 0.05469767975221677, 0.054693909234692674, 0.05469014537866194, 0.05468638817219683, 0.05468263760339178, 0.05467889366036329, 0.05467515633125, 0.05467142560421251, 0.05466770146743334, 0.054663983909116975, 0.054660272917489705, 0.05465656848079964, 0.05465287058731666, 0.05464917922533233, 0.05464549438315985, 0.054641816049134054, 0.05463814421161133, 0.05463447885896955, 0.05463081997960807, 0.05462716756194763, 0.05462352159443039, 0.054619882065519716, 0.054616248963700355, 0.05461262227747823, 0.05460900199538041, 0.054605388105955124, 0.054601780597771675, 0.05459817945942039, 0.05459458467951261, 0.05459099624668059, 0.05458741414957748, 0.0545838383768773, 0.054580268917274875, 0.05457670575948582, 0.05457314889224635, 0.054569598304313474, 0.0545660539844648, 0.054562515921498494, 0.05455898410423328, 0.054555458521508345, 0.05455193916218337, 0.05454842601513845, 0.05454491906927401, 0.05454141831351079, 0.054537923736789846, 0.054534435328072464, 0.0545309530763401, 0.054527476970594374, 0.054524006999857044, 0.054520543153169884, 0.05451708541959473, 0.054513633788213396, 0.054510188248127645, 0.05450674878845912, 0.05450331539834934, 0.054499888066959656, 0.05449646678347117, 0.054493051537084725, 0.05448964231702087, 0.05448623911251984, 0.05448284191284145, 0.054479450707265106, 0.05447606548508972, 0.054472686235633755, 0.054469312948235114, 0.0544659456122511, 0.05446258421705838, 0.05445922875205301, 0.05445587920665035, 0.05445253557028493, 0.05444919783241064, 0.05444586598250044, 0.054442540010046496, 0.05443921990456002, 0.0544359056555714, 0.054432597252629965, 0.05442929468530409, 0.054425997943181044, 0.05442270701586708, 0.05441942189298729, 0.05441614256418564, 0.05441286901912488, 0.05440960124748651, 0.054406339238970806, 0.05440308298329671, 0.054399832470201845, 0.054396587689442416, 0.054393348630793245, 0.05439011528404767, 0.05438688763901759, 0.054383665685533336, 0.0543804494134437, 0.054377238812615865, 0.05437403387293539, 0.054370834584306166, 0.05436764093665037, 0.054364452919908414, 0.05436127052403898, 0.05435809373901896]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'Math': [48, 62, 79, 76, 59],\n",
        "    'Reading': [68, 81, 80, 83, 64],\n",
        "    'Writing': [63, 72, 78, 79, 62]\n",
        "}\n",
        "\n",
        "# Convert the dataset to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Prepare the feature matrix (X) and target vector (Y)\n",
        "X = df[['Math', 'Reading']].values  # Features: Math and Reading scores\n",
        "Y = df[['Writing']].values          # Target: Writing scores\n",
        "\n",
        "# Add a column of ones for the bias term in X\n",
        "X = np.c_[np.ones(X.shape[0]), X]    # Adding bias (intercept term)\n",
        "\n",
        "# Initialize weights (W)\n",
        "W = np.zeros((X.shape[1], 1))  # Initializing weights to zero\n",
        "\n",
        "# Gradient Descent Parameters\n",
        "alpha = 0.01  # Learning rate\n",
        "iterations = 1000  # Number of iterations\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "        loss = Y_pred - Y\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "        W_update = W - alpha * dw\n",
        "        cost = (1 / (2 * m)) * np.sum(loss ** 2)\n",
        "        cost_history[iteration] = cost\n",
        "        W = W_update\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# R Function\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "    return r2\n",
        "\n",
        "# Run gradient descent to optimize weights\n",
        "W_final, _ = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Make predictions using the optimized weights\n",
        "Y_pred = np.dot(X, W_final)\n",
        "\n",
        "# Calculate R\n",
        "r2_value = r2(Y, Y_pred)\n",
        "\n",
        "# Print the R value\n",
        "print(\"R Value:\", r2_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCivFGYb0_Rp",
        "outputId": "97e0d15a-c9c2-4c36-9ef1-7e807933f2b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R Value: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-1d3035722f7c>:38: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum(loss ** 2)\n",
            "<ipython-input-16-1d3035722f7c>:37: RuntimeWarning: invalid value encountered in subtract\n",
            "  W_update = W - alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Main functions to integrate All Steps"
      ],
      "metadata": {
        "id": "h7rCulDE1T4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"Performs gradient descent to optimize weights.\"\"\"\n",
        "    m = len(Y)\n",
        "    cost_history = []\n",
        "    for i in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "        error = Y_pred - Y\n",
        "        cost = (1 / (2 * m)) * np.sum(error**2)\n",
        "        # Stop if cost becomes invalid\n",
        "        if np.isnan(cost) or np.isinf(cost):\n",
        "            print(f\"Gradient Descent stopped at iteration {i} due to invalid cost.\")\n",
        "            break\n",
        "        cost_history.append(cost)\n",
        "        gradient = (1 / m) * np.dot(X.T, error)\n",
        "        W -= alpha * gradient\n",
        "    return W, cost_history\n",
        "\n",
        "def rmse(Y_true, Y_pred):\n",
        "    \"\"\"Calculates the Root Mean Squared Error.\"\"\"\n",
        "    return np.sqrt(mean_squared_error(Y_true, Y_pred))\n",
        "\n",
        "def r2(Y_true, Y_pred):\n",
        "    \"\"\"Calculates the R-Squared value.\"\"\"\n",
        "    return r2_score(Y_true, Y_pred)\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Concepts and technology  of AI/ student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values            # Target: Writing marks\n",
        "\n",
        "    # Normalize the features for numerical stability\n",
        "    X_mean = X.mean(axis=0)\n",
        "    X_std = X.std(axis=0)\n",
        "    X = (X - X_mean) / X_std\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Add a bias term (intercept) to the feature matrix\n",
        "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
        "    X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
        "\n",
        "    # Step 4: Initialize weights (W), learning rate, and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.0001                  # Learning rate\n",
        "    iterations = 1000               # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "    # Experiment with different learning rates\n",
        "    for lr in [0.00001, 0.001, 0.1]:\n",
        "        print(f\"\\nExperimenting with Learning Rate: {lr}\")\n",
        "        W_temp, _ = gradient_descent(X_train, Y_train, np.zeros(X_train.shape[1]), lr, iterations)\n",
        "        if np.isnan(W_temp).any():\n",
        "            print(\"Divergence occurred. Adjust the learning rate.\")\n",
        "            continue\n",
        "        Y_pred_temp = np.dot(X_test, W_temp)\n",
        "        print(f\"RMSE: {rmse(Y_test, Y_pred_temp)}, R: {r2(Y_test, Y_pred_temp)}\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv3Xd8ZV1dii",
        "outputId": "26939654-99fe-43ab-86c1-448f4338d908"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [6.53444381 1.06484994 1.39726956]\n",
            "Cost History (First 10 iterations): [2471.69875, 2471.1904799229724, 2470.6823173626344, 2470.1742622954234, 2469.666314697784, 2469.1584745461646, 2468.65074181702, 2468.14311648681, 2467.6355985320006, 2467.1281879290623]\n",
            "RMSE on Test Set: 63.389626601564274\n",
            "R-Squared on Test Set: -15.05256659905562\n",
            "\n",
            "Experimenting with Learning Rate: 1e-05\n",
            "RMSE: 69.48175118983647, R: -18.28633049576695\n",
            "\n",
            "Experimenting with Learning Rate: 0.001\n",
            "RMSE: 26.083382498908072, R: -1.7179152692809585\n",
            "\n",
            "Experimenting with Learning Rate: 0.1\n",
            "RMSE: 4.788030542881429, R: 0.9084155129155149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, W, learning_rate, iterations):\n",
        "    m = len(Y)\n",
        "    for i in range(iterations):\n",
        "        # Calculate predictions\n",
        "        Y_pred = np.dot(X, W)\n",
        "        # Compute gradients\n",
        "        gradients = -(2/m) * np.dot(X.T, (Y - Y_pred))\n",
        "        # Update weights\n",
        "        W -= learning_rate * gradients\n",
        "    return W\n",
        "\n",
        "# Model Pipeline Function\n",
        "def model_pipeline(file_path, target_column, learning_rate=0.01, iterations=1000):\n",
        "    \"\"\"\n",
        "    This function loads the dataset, splits it, applies gradient descent, and evaluates the model.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: str, path to the CSV file containing the dataset.\n",
        "    - target_column: str, the name of the target column in the dataset.\n",
        "    - learning_rate: float, the learning rate for gradient descent.\n",
        "    - iterations: int, number of iterations for gradient descent.\n",
        "    \"\"\"\n",
        "    # Step 1: Load the data\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 2: Handle missing values\n",
        "    # Check if there are any NaN values\n",
        "    print(\"NaN values in each column before cleaning:\")\n",
        "    print(data.isna().sum())\n",
        "\n",
        "    # Option 1: Drop rows with NaN values (if any)\n",
        "    data = data.dropna()\n",
        "\n",
        "    # Verify no NaN values remain\n",
        "    print(\"NaN values in each column after cleaning:\")\n",
        "    print(data.isna().sum())\n",
        "\n",
        "    # Step 3: Split the data into features (X) and target (Y)\n",
        "    X = data.drop(columns=[target_column]).values  # Feature matrix\n",
        "    Y = data[target_column].values                # Target vector\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Add intercept term (bias) to the feature matrix\n",
        "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]  # Add intercept term (bias)\n",
        "    X_test = np.c_[np.ones(X_test.shape[0]), X_test]     # Add intercept term (bias)\n",
        "\n",
        "    # Check for NaN values in X_train and X_test after adding intercept term\n",
        "    print(\"NaN values in X_train after adding intercept term:\", np.any(np.isnan(X_train)))\n",
        "    print(\"NaN values in X_test after adding intercept term:\", np.any(np.isnan(X_test)))\n",
        "\n",
        "    # Step 5: Define the weight matrix (W) and initialize learning rate and iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights to zeros\n",
        "\n",
        "    # Step 6: Call the gradient descent function to learn the parameters\n",
        "    W = gradient_descent(X_train, Y_train, W, learning_rate, iterations)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R\n",
        "    Y_pred_train = np.dot(X_train, W)\n",
        "    Y_pred_test = np.dot(X_test, W)\n",
        "\n",
        "    # Calculate RMSE and R for training and testing data\n",
        "    rmse_train = np.sqrt(mean_Squared_error(Y_train, Y_pred_train))\n",
        "    rmse_test = np.sqrt(mean_squared_error(Y_test, Y_pred_test))\n",
        "    r2_train = r2_score(Y_train, Y_pred_train)\n",
        "    r2_test = r2_score(Y_test, Y_pred_test)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Training RMSE: {rmse_train}\")\n",
        "    print(f\"Test RMSE: {rmse_test}\")\n",
        "    print(f\"Training R: {r2_train}\")\n",
        "    print(f\"Test R: {r2_test}\")\n",
        "\n",
        "    return W\n",
        "\n",
        "# Example Usage\n",
        "file_path = '/content/drive/MyDrive/Concepts and technology  of AI/ student.csv'  # Path to the CSV file\n",
        "target_column = 'Writing'       # The target column (adjust as per your dataset)\n",
        "\n",
        "# Run the model pipeline\n",
        "model_pipeline(file_path, target_column)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "2MH04-Tm1t2H",
        "outputId": "1da945fc-69c9-4c72-861c-a689d7bd97de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in each column before cleaning:\n",
            "Math       0\n",
            "Reading    0\n",
            "Writing    0\n",
            "dtype: int64\n",
            "NaN values in each column after cleaning:\n",
            "Math       0\n",
            "Reading    0\n",
            "Writing    0\n",
            "dtype: int64\n",
            "NaN values in X_train after adding intercept term: False\n",
            "NaN values in X_test after adding intercept term: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-b3ecfcb65d81>:15: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= learning_rate * gradients\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mean_Squared_error' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b3ecfcb65d81>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Run the model pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-b3ecfcb65d81>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(file_path, target_column, learning_rate, iterations)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Calculate RMSE and R for training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mrmse_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_Squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mrmse_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mr2_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mean_Squared_error' is not defined"
          ]
        }
      ]
    }
  ]
}